<img width="698" height="388" alt="image" src="https://github.com/user-attachments/assets/d4042f41-b7ad-43ac-862d-703a9efa48bc" />


# Linear Regression

선형 회귀는 훈련 셋의 MSE를 최소화할 수 있는 직선의 방정식을 찾는 것

```python
lin_reg = LinearRegression()   # 선형 회귀 모델 생성
```
```python
lin_reg.fit(X_train, y_train)   # 선형 회귀 모델 훈련 - MSE를 최소화하는 직선의 방정식을 찾음.
# 길이와 무게
```
<img width="200" height="67" alt="image" src="https://github.com/user-attachments/assets/1bad0fa9-8f87-461a-aebc-0b1d9461ddb4" />

```python
lin_reg.coef_   # 1차원 배열로 나옴. 선형 회귀식 y = ax + b 에서 독립변수 x의 계수(직선의 기울기) a
```
<img width="147" height="23" alt="image" src="https://github.com/user-attachments/assets/e9675f17-4573-4a0e-ba86-aa09d1449c0a" />

```python
lin_reg.intercept_   # 선형 회귀식 y = ax + b 에서 y 절편 b
```
<img width="224" height="33" alt="image" src="https://github.com/user-attachments/assets/8186ef34-d3b7-4d45-b1fe-d684dc44656d" />

```
y = 39x - 709
```
```python
train_pred = lin_reg.predict(X_train)
```
```python
train_pred  # 물고기 무게가 음수인게 몇개 있음
```
<img width="474" height="179" alt="image" src="https://github.com/user-attachments/assets/fa0f22cd-1cbd-4312-a177-75abecb7e84a" />

```python
X_train @ lin_reg.coef_ + lin_reg.intercept_    # 예측값 찾는거. y = X @ a + b
```
<img width="475" height="181" alt="image" src="https://github.com/user-attachments/assets/84816f53-53f0-4fe1-9c15-f9e352b74ed3" />

```python
# 훈련 셋의 MSE
mean_squared_error(y_train, train_pred)     #> MSE는 KNN의 MSE보다 큼.
```
<img width="130" height="27" alt="image" src="https://github.com/user-attachments/assets/4a4e1a5d-bf3d-4140-9d7e-f83e93360882" />

```python
# 훈련 셋의 결정계수
r2_score(y_train, train_pred)   # 결정계수는 KNN의 결정계수보다 작음.
```
<img width="131" height="31" alt="image" src="https://github.com/user-attachments/assets/2ff38533-4d94-4ec3-a89f-8e8abc115957" />

```
결정계수(R^2)
- 모델의 설명력을 수치로 나타낸 것
- 0부터 1까지의 값을 가지며,
   - 0에 가까울수록 모델이 거의 설명하지 못함
   - 1에 가까울수록 모델이 완벽에 가깝게 설명함
즉, 예측값이 실제값과 얼마나 비슷한지를 나타내는 비율

높다고 항상 좋은 모델은 아님! 과적합 가능성도 고려해야 함
```
```python
# 테스트 셋의 예측값
test_pred = lin_reg.predict(X_test)

# 테스트 셋의 MSE
mean_squared_error(y_test, test_pred)   # KNN의 MSE보다 큼

# 과대적합
```
<img width="140" height="27" alt="image" src="https://github.com/user-attachments/assets/29861f20-1980-477a-a98f-467304c4501c" />

```python
# 테스트 셋의 결정계수
r2_score(y_test, test_pred)     #> KNN 결정계수보다 작음.
```
<img width="145" height="28" alt="image" src="https://github.com/user-attachments/assets/8f1ad52c-d069-4cd2-8476-28b0339f0da6" />

```
Linear Regression 모델의 결과를 보면, 훈련 셋의 평가 점수(MSE, R2)가 테스트 셋의 평가 점수보다 좋음 -> 과적합(overfitting)
```

```python
# 훈련 셋 무게~길이 산점도
plt.scatter(X_train, y_train, alpha = 0.5, label = 'Train')   # alpha는 점의 투명도. 1은 불투명 -> 겹치는 점이 많을 때 더 잘 보이도록 함.

# 테스트 셋 무게~길이 산점도
plt.scatter(X_test, y_test, alpha = 0.5, label = 'Test')

# 가상의 물고기(50cm, 1500g)
plt.scatter(50, 1500, marker = 'v', color = 'red', label = 'Unknown')

# 선형 회귀 직선 선그래프
x = np.arange(5, 51, 0.1).reshape((-1, 1))   # np.arange(5, 51, 0.1)여기까지는 1차원 배열. 2차원 배열로 만들어야 그래프 그려짐
# 5부터 50까지 0.1간격의 숫자 -> 간격이 크면 그래프가 계단처럼 뚝뚝 끊어져 보일 수 있음.
# reshape((-1, 1)) -> 열 수를 1로 만들어라는 의미. 즉, 배열을 1열짜리 2차원 배열로 바꾸는 것
y_hat = x @ lin_reg.coef_ + lin_reg.intercept_  # (n, 1) @ (1,) -> (n,)
plt.plot(x, y_hat, 'r-')
```
