# 농어(Perch) 무게 예측

*   농어의 모든 특성들을 사용한 무게 예측
*   KNN Regressor vs Linear Regression 비교
*   다항 회귀
*   규제(Regularization)

# Imports
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error
```

## 데이터 준비
```python
file_path = 'https://github.com/JakeOh/202505_BD50/raw/refs/heads/main/datasets/fish.csv'
```
```python
file = pd.read_csv(file_path)
```
```python
file.head()
```
<img width="413" height="176" alt="image" src="https://github.com/user-attachments/assets/2fdad08f-1f62-469f-a4b3-7be42fa1835e" />

```python
perch = fish[fish.Species == 'Perch']    # 농어 데이터셋
```
```python
perch.head()
```
<img width="415" height="174" alt="image" src="https://github.com/user-attachments/assets/52c836d4-cd3c-4134-9a3c-9f275610b07b" />

<br>
Weight ~ Length + Diagonal + Height + Width
<br>
```python
# perch.columns[2:]
# X = perch[['Length', 'Diagonal', 'Height', 'Width']].values
X = perch[perch.columns[2:]].values    # 특성(features) 배열
```
```python
X[:5, :]
```
<img width="323" height="86" alt="image" src="https://github.com/user-attachments/assets/a6695c06-18ec-40f5-8dc6-2b3a15302081" />

```python
y = perch['Weight'].values    # 타겟(target) 배열
y[:5]
```
<img width="281" height="38" alt="image" src="https://github.com/user-attachments/assets/1764893d-74b2-48a3-ba3c-29d054467385" />

# 훈련 셋/테스트 셋 나누기
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)    # # shuffle = True(기본값)
```
```python
X_train.shape    # 물고기 42마리, 특성 4개
```
<img width="58" height="31" alt="image" src="https://github.com/user-attachments/assets/2ac3ed82-9bc7-42d4-8b68-6c10c908a030" />

```python
X_test.shape
```
<img width="57" height="28" alt="image" src="https://github.com/user-attachments/assets/d648156f-c9b3-43b0-bf0e-b46a715c9299" />

```python
y_train.shape
```
<img width="49" height="29" alt="image" src="https://github.com/user-attachments/assets/fc8afa8d-3ad2-4cbc-b038-f9e3e5a34ced" />

```python
y_test.shape
```
<img width="52" height="26" alt="image" src="https://github.com/user-attachments/assets/077d12da-a10a-4199-930e-ac704c22edd8" />

# 1차항만 고려한 회귀
## KNN

```python
knn = KNeighborsRegressor()     # ML 모델 생성
```
```python
knn.fit(X_train, y_train)        # ML 모델 훈련
```
<img width="227" height="70" alt="image" src="https://github.com/user-attachments/assets/62ddbcb1-7671-4fa9-a7df-31823b499f5f" />

```python
train_pred = knn.predict(X_train)    # 훈련 셋 예측값 계산
```
```python
train_pred[:5]
```
<img width="307" height="35" alt="image" src="https://github.com/user-attachments/assets/dee78602-f5af-47a8-8d58-60055c85f643" />

```python
y_train[:5]    # 실젯값(농어의 무게)
```
<img width="274" height="30" alt="image" src="https://github.com/user-attachments/assets/0858092a-028f-4fa2-94e5-f038140b0bc7" />

```python
test_pred = knn.predict(X_test)    # 테스트 셋 예측값 계산
```
```python
test_pred[:5]
```
<img width="307" height="30" alt="image" src="https://github.com/user-attachments/assets/f833baa4-dbe9-4458-b29a-31389336fb89" />

```python
y_test[:5]
```
<img width="304" height="28" alt="image" src="https://github.com/user-attachments/assets/6dc41444-36de-403f-8eef-dfa13c9df0c8" />

```python
print('훈련 셋 MSE:', mean_squared_error(y_train, train_pred))
print('훈련 셋 R2:', r2_score(y_train, train_pred))
print('테스트 셋 MSE:', mean_squared_error(y_test, test_pred))
print('테스트 셋 R2:', r2_score(y_test, test_pred))
```
<img width="239" height="73" alt="image" src="https://github.com/user-attachments/assets/897755e3-1fba-48f1-a213-207069b97fa9" />

```
KNN 모델은 과소적합(테스트 셋 점수가 더 좋아서 과소적합 )
```

## Linear Regression

$$
\hat{y} = w_0 + w_1 \times x_1 + w_2 \times x_2 + w_3 \times x_3 + w_4 \times x_4
$$

선형회귀식에서 예측한 값 $\hat{y}$들의 MSE가 최소가 되는 $w_0$ ~ $w_4$를 찾는 과정.

```
# x1 Length, x2 Diagonal, x3 Height, x4 Width, y^ weight

# MSE가 최소가 될수있도록 w0 ~w4 바꿔가면서 찾는 것
```
```python
lin_reg = LinearRegression()    # ML 모델 생성
```
```python
lin_reg.fit(X_train, y_train)    # ML 모델 훈련
```
<img width="217" height="69" alt="image" src="https://github.com/user-attachments/assets/cd044790-490c-47dc-9086-475aa96d0b25" />

```python
lin_reg.coef_
```
<img width="463" height="33" alt="image" src="https://github.com/user-attachments/assets/6dd783b9-5403-4e4b-8f07-e4f844cfd0d2" />

```python
lin_reg.intercept_
```
<img width="230" height="31" alt="image" src="https://github.com/user-attachments/assets/1060581a-6643-4b44-b9a4-cc0c61efb00f" />

무게 = -610 - 40 x Length + 47 x Diagonal + 67 x Height + 35 x Width

```python
train_pred = lin_reg.predict(X_train)   # 훈련 셋 예측값 계산
```
```python
test_pred = lin_reg.predict(X_test)     # 테스트 셋 예측값 계산
```
```python
print('훈련 셋 R2:', r2_score(y_train, train_pred))
print('테스트 셋 R2:', r2_score(y_test, test_pred))
```
<img width="241" height="41" alt="image" src="https://github.com/user-attachments/assets/766d459f-8279-4f25-93cd-9f9cead1102f" />


Linear Regression은 과대적합.

Linear Regression은 KNN보다 오차가 큼.

# 2차항을 고려한 회귀
## KNN

```python
poly = PolynomialFeatures(include_bias=False)
# degree = 2(기본값): 2차항까지 고려
# interaction_only = False(기본값): x1^2,..., x4^2, x1 * x2, x1 * x3, ... 2차항들을 고려.
# 만약 interaction_only = True로 설정하면 제곱항들은 무시됨.

# PolynomialFeatures() : 다항식 형태로 확장해주는 도구
# 다항식 특징을 생성할 때 편향 항(bias term)을 제외하겠다
# include_bias=True -> [1, x, x^2]
# include_bias=False -> [x, x^2]
```
```python
scaler = StandardScaler()

# 표준화를 수행하기 위한 설정(평균 0, 표준편차 1로 만드는 역할)
```
```python
knn = KNeighborsRegressor()
```
```python
pipe = Pipeline(steps = [('poly', poly),
                          ('scaler', scaler),
                         ('knn', knn)])
# 여러 머신러닝 전처리 및 모델 단계를 한 번에 연결하는 구조를 만든 것
# 전처리 → 변환 → 모델 학습 및 예측까지 단일 객체처럼 다룰 수 있음
```
```python
pipe.fit(X_train, y_train)
```
<img width="240" height="180" alt="image" src="https://github.com/user-attachments/assets/95b3f851-655d-4c05-8f98-d508c74c8a15" />

```python
pipe['poly'].get_feature_names_out()
```
<img width="522" height="42" alt="image" src="https://github.com/user-attachments/assets/68b30b03-b3dd-40a7-b051-a8ee5d340b1e" />

```python
train_pred = pipe.predict(X_train)
train_pred[:5]
```
<img width="310" height="31" alt="image" src="https://github.com/user-attachments/assets/9543da59-da0a-413d-ae11-809359940245" />

```python
y_train[:5]
```
<img width="274" height="31" alt="image" src="https://github.com/user-attachments/assets/b159fa04-049e-4f69-97ed-db79be802312" />

```python
test_pred = pipe.predict(X_test)
test_pred[:5]
```
<img width="313" height="29" alt="image" src="https://github.com/user-attachments/assets/93c65220-dece-4a11-acfb-6d449d59d657" />

```python
y_test[:5]
```
<img width="304" height="31" alt="image" src="https://github.com/user-attachments/assets/f125bbb9-418b-480a-b8d9-bcef10f02f26" />

```python
print('훈련 셋 R2', r2_score(y_train, train_pred))
print('테스트 셋 R2', r2_score(y_test, test_pred))
```
<img width="231" height="41" alt="image" src="https://github.com/user-attachments/assets/58c8138b-812e-4216-9bbe-ca9ae0de8507" />

1차항만 고려한 KNN과 비교하면 훈련 셋의 점수가 좋아졌고, 과소적합의 크기도 줄어듦.

## Linear Regression

$$
\hat{y} = w_0 + w_1 \times x_1 + w_2 \times x_2 + w_3 \times x_3 + w_4 \times x_4 + w_5 \times x_1^2 + w_6 \times x_1 x_2 + \dots + w_{14} \times w_4^2
$$

선형 회귀식 예측값들의 MSE를 최소로 하는 $w_0$ ~ $w_{14}$를 찾는 것.

```python
pipe = Pipeline(steps = [('poly', PolynomialFeatures(include_bias=False)),
                         ('scaler', StandardScaler()),
                         ('lin_reg', LinearRegression())])
```
```python
pipe.fit(X_train, y_train)
```
<img width="234" height="173" alt="image" src="https://github.com/user-attachments/assets/6d7fc1ce-2207-4b6c-a609-aac51373d570" />

```python
# 선형 회귀의 계수들(coefficients)
pipe['lin_reg'].coef_    # 총 14개 나옴
```
<img width="431" height="87" alt="image" src="https://github.com/user-attachments/assets/1b13db8d-cf0b-4c96-83bd-dd8dbf632407" />

```python
# 선형 회귀의 절편(intercept)
pipe['lin_reg'].intercept_
```
<img width="209" height="27" alt="image" src="https://github.com/user-attachments/assets/7a037db5-76d3-4e74-86cc-249f9f42d565" />

```python
pipe['poly'].get_feature_names_out()
```
<img width="526" height="40" alt="image" src="https://github.com/user-attachments/assets/574e0860-0212-4273-9eb1-1f607d1e0c81" />

y^ = 400 + (-443)x0 + (1150)x1 + (-650)x2 + ...

```python
train_pred = pipe.predict(X_train)    # 훈련 셋 예측값
train_pred[:5]
```
<img width="443" height="51" alt="image" src="https://github.com/user-attachments/assets/2d4faf09-f427-4c1e-a52f-006ad27d883f" />

```python
y_train[:5]
```
<img width="276" height="34" alt="image" src="https://github.com/user-attachments/assets/07ad6fea-a1a5-454a-bfca-527ce8555a5c" />

```python
test_pred = pipe.predict(X_test)    # 테스트 셋 예측값
```
```python
print('훈련 셋 R2:', r2_score(y_train, train_pred))
print('테스트 셋 R2:', r2_score(y_test, test_pred))
```
<img width="231" height="41" alt="image" src="https://github.com/user-attachments/assets/f6ce75d1-5c97-4bbd-a3d0-df705cc4633f" />

# 규제의 필요성

*   선형 회귀: Weight ~ Length
*   고차항을 포함하는 선형 회귀:
    *   1차항: W ~ L
    *   2차항: W ~ L + L^2
    *   5차항: W ~ L + L^2 + L^3 + L^4 + L^5
    *   50차항: W ~ L + ... + L^50

```python
X = perch[['Length']].values    # 특성 배열(2차원 배열)
y = perch[['Weight']].values    # 타겟 배열
```
```python
# 훈련/테스트 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
```
```python
pipe = Pipeline(steps=[('poly', PolynomialFeatures(degree = 1, include_bias=False)),
                       ('scaler', StandardScaler()),
                       ('reg', LinearRegression())])
```
```python
pipe.fit(X_train, y_train)
```
<img width="232" height="176" alt="image" src="https://github.com/user-attachments/assets/4aee9d72-7cdb-4c54-8fa6-7acca1bb4c00" />

```python
pipe['reg'].coef_
```
<img width="179" height="25" alt="image" src="https://github.com/user-attachments/assets/bcf159f7-95b7-402b-8453-4bac5c9004fc" />

```python
pipe['reg'].intercept_
```

y = 400 + 340 x X

```python
# 훈련 셋 산점도(무게 ~ 길이)
plt.scatter(X_train, y_train, color = 'DarkGray', alpha = 0.5, label = 'train')

# 테스트 셋 산점도
plt.scatter(X_test, y_test, color = 'Red', alpha = 0.5, label = 'test')

# 선형 회귀 직선 선그래프
x_vals = np.arange(7, 45, 0.001).reshape((-1, 1))
y_vals = pipe.predict(x_vals)    # y = w0 + w1 x x. 선형 회귀 모델을 통해 무게 예측
plt.plot(x_vals, y_vals, label = 'degree = 1')    # 직선으로 연결된 그래프 (회귀선)

plt.legend()
plt.grid()
plt.xlabel('Length (cm)')
plt.ylabel('Weight (g)')
plt.show()
```
<img width="531" height="391" alt="image" src="https://github.com/user-attachments/assets/837adb2b-3dab-4967-970c-241c6dd19ed7" />

```python
plt.figure(figsize = (10, 10))

# 훈련 셋 산점도
plt.scatter(X_train, y_train, color = 'black', alpha = 0.5, label = 'train')

# 테스트 셋 산점도
plt.scatter(X_test, y_test, color = 'orange', alpha = 0.5, label = 'test')

# PolynomialFeatures에서 사용할 degree(차수) 값들
degrees = (1, 2, 5, 50)
for d in degrees:
    # Pipeline 객체 생성
    pipe = Pipeline(steps=[('poly', PolynomialFeatures(degree = 1, include_bias=False)),
                       ('scaler', StandardScaler()),
                       ('reg', LinearRegression())])
    # 훈련 셋으로 ML 모델 훈련
    pipe.fit(X_train, y_train)

    # 훈련 셋/테스트 셋 MSE 출력
    print(f'----- degree = {d} -----')
    train_pred = pipe.predict(X_train)
    print('훈련 셋 MSE =', mean_squared_error(y_train, train_pred))
    test_pred = pipe.predict(X_test)
    print('테스트 셋 MSE =', mean_squared_error(y_test, test_pred))

    # 회귀 식을 시각화하기 위해서
    x_vals = np.arange(7, 45, 0.001).reshape((-1, 1))   # 회귀식 시각화를 위한 x좌표들
    y_vals = pipe.predict(x_vals)   # 회귀식 시각화를 위한 y좌표들
    plt.plot(x_vals, y_vals, label = f'degree = {d}')   # 선그래프

plt.legend()
plt.grid()
plt.xlabel('Length (cm)')
plt.ylabel('Weight (g)')
plt.ylim((-100, 1200))      # y축을 그릴 범위를 제한
plt.show()    
```
<img width="793" height="958" alt="image" src="https://github.com/user-attachments/assets/4217e9a7-97e8-4b15-8a19-dca9363d6c41" />


# 규제

*   머신 러닝의 목적은 비용 함수(cost function)/손실 함수(loss function)을 최소화하는 것.

*   선형 회귀(linear regression)의 목적은 MSE(mean_squared_error)를 최소로 만드는 계수들($w_0, ..., w_m$)을 찾는 것. 즉, 선형 회귀의 비용 함수는 MSE(w).

$$
J(w) = MSE(w)
     = \dfrac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2
     = \dfrac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + \sum_{j=1}^{m} w_{j} \cdot x_{ij}))^2
$$

    *   $N$: 샘플의 개수(DataFrame의 row의 개수)
    *   $m$: 특성의 개수(DataFrame의 column의 개수)
    *   $y_i$: $i$번째 샘플의 실젯값
    *   $\hat{y_i}$: $i$번째 샘플의 예측값
    *   $x_{ij}$: $i$번째 샘플의 $j$번째 특성 값
    *   $w_j$: $j$번째 특성에 곱해주는 계수(기울기)

*   Ridge(`l2`) 규제의 목적은 아래의 비용 함수를 최소로 만드는 계수들($w_0, ..., w_m$)을 찾는 것.

$$
J(w) = MSE(w) + \dfrac{\alpha}{2} \mid\mid w \mid\mid ^2
$$

> Ridge 규제의 효과는 고차항들의 계수(coefficients, 기울기)를 작게 만들어서 곡선의 기울기를 완만하게 만들어 주게 됨. overfitting(과대적합)을 줄여주게 됨.

*   Lasso(`l1`) 규제의 목적은 아래의 비용 함수를 최소로 만드는 계수들($w_0, ..., w_m$)을 찾는 것.

$$
J(w) = MSE(w) + \alpha \mid\mid w \mid\mid
$$

> Lasso 규제는 중요하지 않은 고차항들의 계수를 0으로 만들어 줌으로써, 곡선을 완만하게 만들어 주는 효과가 생김. 과대적합을 줄여주는 효과.

*   ElasticNet 규제의 목적은 아래의 비용 함수를 최소로 만드는 계수들($w_0, ..., w_m$)을 찾는 것.

$$
J(w) = MSE(w) + r \alpha \mid\mid w \mid\mid
              + \dfrac{1 - r}{2} \alpha \mid\mid w \mid\mid ^2
$$

> ElasticNet은 Ridge 규제와 Lasso 규제의 효과를 모두 가지고 있음.

*   Ridge, Lasso에서 $\alpha$ 값이 크면 규제가 강해지고, $\alpha$ 값이 작으면 규제가 약해짐($ \alpha \ge 0 $).
*   ElasticNet에서 $\alpha$는 규제의 크기를 의미하고, $r$은 `l1` 규제의 비율($0 \le r \le 1$)을 의미함.

## L2 규제(Ridge)

```python
def visualize_regression(X_train, y_train, X_test, y_test, degress, estimator):
    plt.figure(figsize = (10, 10))

    plt.scatter(X_train, y_train, color = 'black', alpha = 0.5, label = 'train')
    plt.scatter(X_test, y_test, color = 'orange', alpha = 0.5, label = 'test')
    for d in degrees:
        pipe = Pipeline(steps=[('poly', PolynomialFeatures(degree=d, include_bias=False)),
                               ('scaler', StandardScaler()),
                               ('reg', estimator)])
        pipe.fit(X_train, y_train)
        train_pred = pipe.predict(X_train)
        test_pred = pipe.predict(X_test)
        print(f'degree={d}: {r2_score(y_train, train_pred)}(Train) / {r2_score(y_test, test_pred)}(Test)')
        x_vals = np.arange(7, 45, 0.001).reshape((-1, 1))
        y_vals = pipe.predict(x_vals)
        plt.plot(x_vals, y_vals, label = f'degree = {d}')

    plt.grid()
    plt.legend()
    plt.xlabel('Length (cm)')
    plt.ylabel('Weight (g)')
    plt.ylim((-100, 1200))
    plt.show()
```
```python
degrees = (1, 2, 5, 50)
estimator = Ridge()
visualize_regression(X_train, y_train, X_test, y_test, degrees, estimator)
```
<img width="790" height="830" alt="image" src="https://github.com/user-attachments/assets/13cb2b72-e69b-4ed8-b502-2c2cd453e569" />

## L2 규제(Ridge)

```python
visualize_regression(X_train, y_train, X_test, y_test, degrees, Lasso())
```
<img width="781" height="855" alt="image" src="https://github.com/user-attachments/assets/d673743a-4336-482d-b106-39afcab5a6c5" />

## ElasticNet

```python
visualize_regression(X_train, y_train, X_test, y_test, degrees, ElasticNet())
```
<img width="796" height="833" alt="image" src="https://github.com/user-attachments/assets/1d6c3c19-7f18-47ab-bbfb-952e72f5bb31" />

# 하이퍼파라미터(Hyper-parameter) 튜닝

*   모델 파라미터(model parameter): 머신 러닝 알고리즘에서 만들어진 수학적인 모델의 비용함수를 최소화하도록 만드는 파라미터들. 머신 러닝에서 스스로 찾아내는 값들.
*   하이퍼파라미터(hyper-parameter): 머신 러닝 알고리즘에서 개발자 바꿔가면서 테스트하는 파라미터들.
    *   PolynomialFeatures의 degree
    *   규제(L2, L1, ElasticNet)에서 alpha
    *   KNN 알고리즘 k


