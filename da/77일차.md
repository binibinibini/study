# LSTM(Long Short-Term Memory, 장/단기 기억)

*   단기 기억을 오래 기억하기 위해서 고안된 순환 신경망.
*   LSTM 셀 한 개는 입력 게이트, 삭제 게이트, 출력 게이트를 가지고 있음.
    *   입력 게이트: 새로운 정보를 셀 상태(cell state)에 추가. 단기 기억 저장.
    *   삭제 게이트: 셀 상태에 있는 정보를 제거. 장기 기억을 삭제.
    *   출력 게이트: 다음 은닉 상태(hidden state)로 출력을 내보냄.
    *   LSTM 셀 한 개는 작은 순환 신경 셀 4개로 이루어진 구조. (입력 게이트 2, 삭제 게이트 1, 출력 게이트 1)
 

```python
tf.random.set_seed(42)
np.random.seed(42)

model_lstm = keras.Sequential(layers = [
    keras.Input(shape = (100, )),    # 모델 입력 형태 (100,)
    keras.layers.Embedding(input_dim = 500, output_dim = 32),  # 500개의 단어 사용, 32차원의 벡터로 변환
    keras.layers.LSTM(units = 32),  # 32개의 LSTM 유닛 사용
    keras.layers.Dense(units = 1, activation = 'sigmoid')  # 0 ~ 1 사이의 확률값으로 변환
])

model_lstm.summary()
```
<img width="588" height="207" alt="image" src="https://github.com/user-attachments/assets/4e7d4601-e973-4f8e-95c5-45fe4a2effec" />

```
500 * 32 = 16000

LSTM 층의 모델 파라미터 개수 = (embedding input + 순환입력 + 바이어스) * 게이트(4개) * 전체 셀의 개수
(32 + 32 + 1 ) * 4 * 32
```
```python
keras.utils.plot_model(model = model_lstm, show_shapes = True, dpi = 128)
```
<img width="704" height="590" alt="image" src="https://github.com/user-attachments/assets/d7d29a17-8568-49bb-83bd-1b6704d43453" />

```python
model_lstm.compile(optimizer = keras.optimizers.Adam(),
                   loss = keras.losses.binary_crossentropy,
                   metrics = [keras.metrics.binary_accuracy])
```
```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = 'lstm.keras', save_best_only = True)
early_stop = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
```
```python
result = model_lstm.fit(x = x_train, y = y_train, batch_size = 64, epochs = 100,
                        callbacks = [checkpoint, early_stop],
                        validation_data = [x_val, y_val])
```
<img width="1092" height="422" alt="image" src="https://github.com/user-attachments/assets/d28c00bd-6852-49c5-bdab-d45b7ea9ce13" />

```python
plot_train_val_loss(result)
```
<img width="575" height="432" alt="image" src="https://github.com/user-attachments/assets/af5a1f76-7175-4f13-9f2d-9137fd4e9a48" />

```
3번 기다렸는데 최솟값보다 작은 값 안나와서 종료
```

```python
model_lstm.evaluate(x = x_train, y = y_train)
```
<img width="737" height="41" alt="image" src="https://github.com/user-attachments/assets/0b2a2731-d79d-4759-8064-22481e27bf1a" />

```
20000/625
```

```python
model_lstm.evaluate(x = x_val, y = y_val, batch_size=64)
```
<img width="716" height="43" alt="image" src="https://github.com/user-attachments/assets/9227a0bd-442d-4ca9-b965-06deccd10c55" />

```
5000 / 64 = 78.125 -> 79
```

## Drop-out을 적용한 LSTM

```python
model_lstm2 = keras.Sequential(layers = [
    keras.Input(shape = (100, )),
    keras.layers.Embedding(input_dim = 500, output_dim = 32),  # 자주 사용하는 단어 500개 사용
    keras.layers.LSTM(units = 32, dropout = 0.2),
    keras.layers.Dense(units = 1, activation = 'sigmoid)
])
```
```python
model_lstm2.summary()
```
<img width="572" height="208" alt="image" src="https://github.com/user-attachments/assets/c681a595-52a7-4742-addf-1112c977ffa3" />

```
dropout 추가해도 위에 모델이랑 파라미터 개수가 같음. 영향을 미치지 않음.
```

```python
model_lstm2.compile(optimizer = keras.optimizers.Adam(),
                    loss = keras.losses.binary_crossentropy,
                    metrics = [keras.metrics.binary_accuracy])
```
```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = 'lstm_dropout.keras',
                                              save_best_only = True)
early_stop = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
```
```python
result = model_lstm2.fit(x = x_train, y = y_train, batch_size = 32, epochs = 100,
                         callbacks = [checkpoint, early_stop],
                         validation_data = [x_val, y_val])
```
<img width="1083" height="352" alt="image" src="https://github.com/user-attachments/assets/2936a6c8-c163-444f-b1f1-c63d74dea54d" />

```python
plot_train_val_loss(result)
```
<img width="584" height="432" alt="image" src="https://github.com/user-attachments/assets/617125ba-6171-4cae-97fa-15fb1f3e11f8" />

```python
model_lstm2.evaluate(x = x_train, y = y_train)
```
<img width="725" height="42" alt="image" src="https://github.com/user-attachments/assets/17a436ce-a63d-4d48-9c60-96af7a693742" />

```python
model_lstm2.evaluate(x = x_val, y = y_val)
```
<img width="720" height="44" alt="image" src="https://github.com/user-attachments/assets/3c5d590a-e385-4032-8e0e-47150c4ffc6f" />


## 2개의 LSTM 층 연결

*   Embedding --> LSTM(1) --> LSTM(2) --> 출력층
*   Embedding 층의 output_dim은 32로 설정.
*   LSTM(1)의 셀의 개수는 32개. dropout은 0.2로 설정.
    *   return_sequence 파라미터 값을 설정.
*   LSTM(2)의 셀의 개수와 dropout 비율은 첫번째와 동일하게.
    *   return_sequence 파라미터 값은 기본 값(default argument)을 사용.
 
```python
model_lstm3 = keras.Sequential(layers = [
    keras.Input(shape = (100, )),
    keras.layers.Embedding(input_dim = 500, output_dim = 32),
    keras.layers.LSTM(units = 32, dropout = 0.2, return_sequences = True),
    keras.layers.LSTM(units = 32, dropout = 0.2),
    keras.layers.Dense(units = 1, activation = 'sigmoid')
])
```
```python
model_lstm3.summary()
```
<img width="547" height="234" alt="image" src="https://github.com/user-attachments/assets/3616c5c2-6fe7-4a2d-ad46-bafe2a0bc1d7" />

```python
keras.utils.plot_model(model = model_lstm3, show_shapes = True, dpi = 128)
```
<img width="745" height="804" alt="image" src="https://github.com/user-attachments/assets/06c7d023-ef57-414a-8c5e-434b8b528ab7" />

```
2개 이상의 LSTM 층을 연결할 때, 
*   원래 순환층(RNN, LSTM)의 은닉 상태는 샘플의 마지막 타임스텝에 대한 은닉 상태만 그 다음 층으로 전달.
*   순환층을 2개 이상 쌓게 되면 모든 순환층에서 순차 데이터(시퀀스)가 필요.
*   따라서 앞쪽 순환층이 모든 타임스텝에 대한 은닉 상태를 출력해야 함. 시퀀스를 다음 순환층으로 입력해줘야 함.
*   오직 마지막 순환층에서만 마지막 타임스텝의 은닉 상태를 출력해야 함.
*   중간에 포함된 순환층에서는 return_sequences = True로 설정해야 함.
*   마지막 순환층에서만 return_sequences = False (기본값)으로 설정해야 함.
```

```python
model_lstm3.compile(optimizer = keras.optimizers.Adam(),
                    loss = keras.losses.binary_crossentropy,
                    metrics = [keras.metrics.binary_accuracy])
```
```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = 'lstm_2.keras',
                                             save_best_only=True)
early_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
```
```python
result = model_lstm3.fit(x = x_train, y = y_train, batch_size = 32, epochs = 100,
                         callbacks=[checkpoint, early_stop],
                         validation_data = [x_val, y_val])
```
<img width="1096" height="194" alt="image" src="https://github.com/user-attachments/assets/7d434627-f7eb-4d16-aeb1-3380f484d5c5" />

```python
plot_train_val_loss(result)
```
<img width="576" height="432" alt="image" src="https://github.com/user-attachments/assets/70abc0af-5aab-4dbc-ba88-2a2a03b10430" />

```python
model_lstm3.evaluate(x = x_train, y = y_train)
```
<img width="744" height="39" alt="image" src="https://github.com/user-attachments/assets/22edc7d4-cd7e-4e4d-a5fb-89e682bdaa5d" />

```python
model_lstm3.evaluate(x = x_val, y = y_val)
```
<img width="735" height="39" alt="image" src="https://github.com/user-attachments/assets/603985d1-2445-43e8-aa9c-6c4136efc078" />


# GRU(Gated Recurrent Unit)

*   3개의 작은 셀들로 이루어진 순환 셀. 2개는 sigmoid, 1개는 tanh를 활성화 함수로 사용.
*   LSTM보다 훈련을 통해서 찾는 모델 파라미터 개수가 작아짐. 계산량이 작아짐.
*   일반적으로 LSTM보다 더 좋은 성능을 냄.


```
Embedding --> GRU --> Dense
```
```python
model_gru = keras.Sequential(layers = [
    keras.Input(shape = (100, )),
    keras.layers.Embedding(input_dim = 500, output_dim = 32),
    keras.layers.GRU(units = 32, dropout = 0.2),
    keras.layers.Dense(units = 1, activation = 'sigmoid')
])

model_gru.summary()
```
<img width="556" height="207" alt="image" src="https://github.com/user-attachments/assets/0eb7536b-faa8-4d62-b092-01833079bc80" />

```
GRU 층에서 학습되는 모델 파라미터 개수
(embedding inputs * 순환 은닉 입력 + 바이어스) * 게이트(3) * GRU 셀 개수(32) + keras 구현
(32 + 32 + 1) * 3 * 32 + (2 + 1) * 32
```
```python
model_gru.compile(optimizer = keras.optimizers.Adam(),
                  loss = keras.losses.binary_crossentropy,
                  metrics = [keras.metrics.binary_accuracy])
```
```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = 'model_gre.keras', save_best_only = True)
early_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
```
```python
result = model_gru.fit(x = x_train, y = y_train, batch_size = 64, epochs = 100,
                      callbacks = [checkpoint, early_stop],
                      validation_data = [x_val, y_val])
```
<img width="1103" height="384" alt="image" src="https://github.com/user-attachments/assets/db348813-4395-4be9-9559-8e472d2b4841" />

```python
plot_train_val_loss(result)
```
<img width="575" height="432" alt="image" src="https://github.com/user-attachments/assets/6b223c5c-7c63-42be-a269-bffb9ee053c4" />

```python
model_gru.evaluate(x = x_train, y = y_train)
```
<img width="728" height="42" alt="image" src="https://github.com/user-attachments/assets/a5203c12-c02c-43a1-ba7a-a28e18c8d5f3" />

```python
model_gru.evaluate(x = x_val, y = y_val)
```
<img width="731" height="42" alt="image" src="https://github.com/user-attachments/assets/ef8c0634-92c6-4a18-85e9-34fafc1b6eea" />
