# 규제 크기(alpha)에 따른 결정계수(R2 score) 변화

*   특성: Length, Diagonal, Height, Width, 레이블: Weight
*   Ridge, degree = 5, StandardScaler, alpha = (0.001, 0.01, 0.1, 1.0, 10, 100), 훈련/테스트 R2
*   Lasso, degree = 5, StandardScaler, alpha = (0.001, 0.01, 0.1, 1.0, 10, 100), 훈련/테스트 R2
*   ElasticNet, degree = 5, StandardScaler, alpha = (0.001, 0.01, 0.1, 1.0, 10, 100), 훈련/테스트 R2

```python
X = perch[perch.columns[2:]].values
y = perch['Weight'].values
```
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
```
```python
def visualize_r2_score(X_tr, y_tr, X_te, y_te, estimator, alphas):    # estimator: 머신러닝 모델, alphas: 테스트할 알파값
    train_scores = []    # 규제 크기(alpha)에 따른 훈련 셋의 결정 계수를 저장할 리스트
    test_scores = []     # 규제 크기(alpha)에 따른 테스트 셋의 결정 계수를 저장할 리스트
    for a in alphas:
        if estimator == 'l1':
            reg = Lasso(alpha = a)
        elif estimator == 'l2':
            reg = Ridge(alpha = a)
        elif estimator == 'elastic':
            reg = ElasticNet(alpha = a)
        else:
            raise ValueError('estimator는 "l1", "l2", 또는 "elastic"이어야 함.')

        pipe = Pipeline(steps = [('poly', PolynomialFeatures(degree = 5, include_bias = False)),
                                ('scaler', StandardScaler()),
                                ('reg', reg)])
        pipe.fit(X_tr, y_tr)

        train_r2 = pipe.score(X_tr, y_tr)
        train_scores.append(train_r2)
        test_r2 = pipe.score(X_te, y_te)
        test_scores.append(test_r2)

        print(f'alpha={a}: train({train_r2:.3f}) / test({test_r2:.3f})')    # 3f: 소수점 이하는 3자리까지 출력

    # 그래프 그리는 건 반복문 밖에
    plt.plot(np.log10(alphas), train_scores, 'bo-', label='train $r^2$')    # np.log10(alphas): log값으로 사용한 이유는 알파 값을 0.001, 0.01, 0.1.. 사용할거라서
    plt.plot(np.log10(alphas), test_scores, 'r^:', label='test $r^2$')
    plt.legend()
    plt.grid()
    plt.xlabel('alpha (log scale)')
    plt.ylabel('$r^2$ score')
    plt.show()
```
```python
alphas = [10 ** x for x in range(-3, 3)]
alphas
```
<img width="219" height="33" alt="image" src="https://github.com/user-attachments/assets/1adf1326-44ff-4a50-a077-ab6d4a32a67a" />

```python
visualize_r2_score(X_train, y_train, X_test, y_test, 'l1', alphas)
```
<img width="647" height="602" alt="image" src="https://github.com/user-attachments/assets/029c43c2-7380-40b4-9c8d-6a92a402f2d9" />

```
alpha가 1일때 제일 좋음(alpha는 규제강도를 조절하는 하이퍼파라미터)
규제가 너무 커서 훈련셋, 테스트 셋 오차가 큼.
훈련 성능은 alpha 증가에 따라 점진적으로 감소함 -> 모델이 복잡한 패턴을 덜 학습함.
테스트 성능은 비슷하다가 alpha = 100에서 급격히 떨어짐
```

```python
visualize_r2_score(X_train, y_train, X_test, y_test, 'l2', alphas)
```
<img width="534" height="489" alt="image" src="https://github.com/user-attachments/assets/6c361b59-9866-4567-b4dc-f1896e5575e3" />

```
규제가 작을 때 훈련 셋만 높음
규제를 조금 더 크게했을 때, 훈련셋은 조금 낮아지는데 테스트 셋은 올라감
과대적합이 컸다가 훅 떨어지는 경우가 있음
규제가 커지면 오차가 커진다는거.
알파값 10^(-1)일때가 가장 좋다
```

```python
visualize_r2_score(X_train, y_train, X_test, y_test, 'elastic', alphas)
```
<img width="633" height="625" alt="image" src="https://github.com/user-attachments/assets/77c96384-4a06-401a-88f0-c2b06cb857ff" />

```
10^(-3), 10^(-2)가 좋을 듯
1보다 큰 값은 안좋음
```

--------

# Gradient Descent(경사하강법)

*   머신 러닝의 목적은 비용(손실) 함수를 최소로 만드는 파라미터들(w0, w1, w2, ...)을 찾는 것.
*   회귀 문제인 경우 비용(손실) 함수는 MSE(w).
    *   회귀 문제에서는 w에 대한 2차 함수의 최솟값의 위치를 찾는 문제와 비슷.   
*   경사 하강법: 최솟값의 위치를 찾는 알고리즘 중의 하나.
    *   비용(손실)함수의 임의의 위치에서 시작.
    *   그 위치에서 접선의 기울기(gradient)를 계산.
    *   gradient의 절대값이 줄어드는 방향으로 w 값을 변경.
    *   위의 과정을 충분히 반복하면 비용(손실) 함수가 최소가 되는 위치 w를 찾을 수 있음.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```
```python
w = np.arange(-5, 5, 0.0001)
f = w ** 2    # 비용함수

plt.plot(w, f)
plt.grid()
plt.xlabel('w')
plt.ylabel('cost function')
plt.show()
```
<img width="513" height="392" alt="image" src="https://github.com/user-attachments/assets/ee218545-8f10-4161-944b-54b2e2ab5ad4" />

```python
# 임의의 위치 w를 선택
w_init = 4

# 선택한 w에서 비용(손실) 함수의 gradient(접선의 기울기) 계산
gradient = 2 * w_init    # 만든 함수가 f = w^2이기 때문에 미분하면 f' = 2w가 접선의 기울기

# 처음 위치에서 기울기의 반대 방향으로 약간 이동
w_next = w_init - gradient * 0.1    # 0.1만큼 이동

w_next
```
<img width="38" height="30" alt="image" src="https://github.com/user-attachments/assets/a0295b42-b955-4524-b517-c333b6f77dbd" />

```python
# 이동한 점에서 시작
w_init = w_next

# 선택한 w에서 비용(손실) 함수의 gradient(접선의 기울기) 계산
gradient = 2 * w_init

# 처음 위치에서 기울기의 반대 방향으로 약간 이동
w_next = w_init - gradient * 0.1

w_next
```
<img width="48" height="30" alt="image" src="https://github.com/user-attachments/assets/77e335c0-fcbb-47b1-87e3-1592db77fb0e" />

```
점점 0에 가까워짐. (w가 최솟값을 향해 가고 있음 -> 경사하강법)
최솟값(기울기 0) 위치에 도달하게 되면 값이 바뀌지 않음. 
gradient * 여기에 곱하는 숫자에 따라서 최솟값에 도달하는 속도가 달라짐
```

```python
def gradient_descent(learning_rate = 0.1, max_iter = 1_000, tolerance = 0.0001):    # learning_rate: 학습률, max_iter: 최대 반복 횟수 
    # 손실 함수 시각화
    w = np.arange(-5, 5, 0.0001)    # 1차원 배열을 만든 거
    f = w ** 2
    plt.plot(w, f, color = 'DarkGray')

    # w가 얼마일 때 함수 f가 최소일까?
    # 임의의 w 값에서 시작
    w_init = -4    # 원래는 난수로 만듦
    # 시작 w의 위치를 점으로 표시
    plt.scatter(w_init, w_init ** 2, label = 'o')

    convergence = False
    for n in range(max_iter):
        # 시작값에서 접선의 기울기 계산
        gradient = 2 * w_init
        # gradient가 감소하는 방향으로 w 값을 변경
        w_next = w_init - gradient * learning_rate
        # 이동한 점의 위치를 점으로 표시
        plt.scatter(w_next, w_next ** 2, label = f'{n + 1}')
        # 반복을 계속 할 지 멈출 지를 결정.
        if np.abs(w_next - w_init) < tolerance:    # np.abs() : 절댓값 리턴
            # 이동한 w의 위치와 처음 w 위치 사이의 거리가 0.0001 미만이면
            convergence = True
            break    # 반복 그만
        # 다음 이동 위치를 계산하기 위해서 바뀐 현재 위치를 새로운 초기 위치로 변경.
        w_init = w_next

    if convergence == False:
        print('최솟값으로 수렴하지 않았을 수도 있음.')

    plt.legend()
    plt.grid()
    plt.xlabel('w')
    plt.ylabel('cost function')
    plt.show()
```
```python
gradient_descent()
# 42번해야 최솟값 도달
```
<img width="522" height="874" alt="image" src="https://github.com/user-attachments/assets/21e058ac-c17d-48e6-8cb1-9563c99f4dc7" />

```python
gradient_descent(max_iter=5)
# 5번 반복.
# 최초 위치가 -4
```
<img width="523" height="415" alt="image" src="https://github.com/user-attachments/assets/23969bfd-289b-4507-9fd2-b0fcdfd9b678" />

```python
gradient_descent(max_iter=10)
# 거의 최솟값까지 감
```
<img width="525" height="412" alt="image" src="https://github.com/user-attachments/assets/2756bb43-c6b7-41cb-a12e-2a5437c57c65" />

```python
gradient_descent(learning_rate=0.2)
# 20번만에 끝남
```
<img width="516" height="431" alt="image" src="https://github.com/user-attachments/assets/e6fd9b97-60d8-4a3a-9b1b-8fc9bcaf3fe3" />

```python
gradient_descent(learning_rate=0.8)
# 순서는 옆에 표 보면 됨.
```
<img width="510" height="486" alt="image" src="https://github.com/user-attachments/assets/6b0d09da-6b99-46cb-8f9b-032edaf1ac60" />

```python
gradient_descent(tolerance=0.1)
# |Wnext - Winit| < 0.1이라서 11번만에 끝남
# tolerance -> 반복 과정에서 값이 얼마나 작게 변해야 멈출지를 정하는 기준값
```

```python
gradient_descent(learning_rate=1.0, max_iter = 10)
```
<img width="512" height="385" alt="image" src="https://github.com/user-attachments/assets/7198c826-164f-482e-adbe-1c707d2a8072" />

```python
gradient_descent(learning_rate=1.1, max_iter=10)
```
<img width="533" height="410" alt="image" src="https://github.com/user-attachments/assets/63c52330-417b-4fc4-ba99-5249f3d03ba8" />


**학습률(learning rate) 하이퍼 파라미터**

*   학습률이 작은 경우에는 최솟값을 향해서 천천히 움직임.
*   학습률이 큰 경우에는 최솟값을 향해서 빠르게 또는 불안정하게 움직임.
*   학습률이 너무 작으면 최대 반복 횟수(max_iter) 안에서 최솟값으로 수렴(convergence)하지 못할 수도 있음.
    *   학습률을 키워줌.
    *   최대 반복 횟수를 늘려줌.
    *   수렴 반경(tolerance)을 늘려줌. -> 권장하진 않음
*   학습률이 너무 크면 수렴하지 못하고 발산(divergence)하는 경우가 생기기도 함.
    *   학습률을 줄여야 함.
*   ML 러닝 알고리즘들 중에서는 처음에는 학습률을 크게 하고, 에포크(epoch, 반복)가 진행될 때마다 학습률을 점점 줄여나가는 방식으로 데이터를 학습하는 알고리즘도 있음.

# SGD(Stochastic Gradient Descent, 확률적 경사 하강법)

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDRegressor 
```
```python
# 데이터 셋 
file_path = 'https://github.com/JakeOh/202505_BD50/raw/refs/heads/main/datasets/fish.csv'
```
```python
fish = pd.read_csv(file_path)
```
```python
perch = fish[fish.Species == 'Perch']
perch.columns
```
<img width="613" height="34" alt="image" src="https://github.com/user-attachments/assets/8ce5dba8-4afe-47f0-8388-42f50477b269" />

```python
X = perch[perch.columns[2:]].values    # 특성 배열
y = perch['Weight'].values    # 타겟 배열
```
```python
# 훈련 셋/테스트 셋 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
```
```python
sgd = SGDRegressor(random_state = 42)    # ML 모델 생성
```
```python
sgd.fit(X_train, y_train)    # ML 모델 훈련 -> 비용(손실) 함수를 최소로 만드는 w들을 찾음.
```
<img width="228" height="72" alt="image" src="https://github.com/user-attachments/assets/e678f934-5ed6-46df-830f-864d08262109" />

```python
sgd.score(X_train, y_train)     # 훈련 셋에서의 결정계수(r2 score)
# 결과가 안좋음!
```
<img width="168" height="30" alt="image" src="https://github.com/user-attachments/assets/05b998e2-25ab-45f2-8054-043b40811d5e" />

```python
sgd.score(X_test, y_test)       # 테스트 셋에서의 결정계수
```
<img width="179" height="33" alt="image" src="https://github.com/user-attachments/assets/007a139e-4bda-4428-ac64-1f2460b0c24c" />

훈련 셋의 샘플 개수가 너무 작아서 1,000번 반복(epoch)만으로는 최적의 값으로 수렴이 되지 않음.

해결 방법:
*   학습률을 키움.
*   반복 횟수를 늘려줌.
*   규제의 크기를 줄임.

*   SGDRegressor 사용
*   max_iter = 1_000_000_000
*   learning_rate = ['constant', 'invscaling', 'adaptive']
*   eta0 = [0.01, 0.1, 1.0, 10]     # 학습률의 초기값
*   learning_rate와 eta0를 바꾸면서 훈련 셋과 테스트 셋에서의 결정계수를 출력

```python
max_iter = 1_000_000_000    # 최대 반복(epoch) 횟수
learning_rate = ['constant', 'invscaling', 'adaptive']    # 학습률 적용 방법
etas = [0.01, 0.1, 1.0, 10]     # 학습률 초깃값

for lr in learning_rate:
    for e in etas:
        sgd = SGDRegressor(random_state = 42, max_iter = max_iter, learning_rate = lr, eta0 = e)
        sgd.fit(X_train, y_train)
        train_score = sgd.score(X_train, y_train)
        test_score = sgd.score(X_test, y_test)
        print(f'learning rate = {lr}, eta0 = {e}, train r2 = {train_score}, test r2 = {test_score}')
```
<img width="783" height="220" alt="image" src="https://github.com/user-attachments/assets/22665deb-a6d9-4314-9dce-631470f04ad1" />

```
constant -> 계속 같은 걸음으로 이동. 좋은 조건에선 빠르지만, 큰 값이면 발산할 가능성이 있음.
invscaling -> 시간이 지날수록 점점 안정적인 업데이트가 가능. 전체 반복 횟수가 많을 때 유리
adaptive -> 일정 에폭 동안 성능이 좋아지지 않으면, 자동으로 조정. 손쉽게 튜닝하면서 안정적인 수렴을 도와줌.

위에 결과에서는
eta0 = 1.0, eta0 = 10과 같은 큰 값은 학습률이 너무 큼.(발산하기 좋은 조건)
max_iter = 1_000_000_000과 같이 반복 횟수는 너무 크고, 거기다 학습률이 크면 모델이 조금만 학습해도 극단적인 값으로 치솟을 수 있음
```

-------------
# Classification(분류)

*   Fish 데이터셋에서 Bream/Smelt 분류 문제

```
# 머신러닝
# 지도/비지도
# 사례기반(KNN)/모델기반(LR)
```

# Imports
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
```

# 데이터셋

```python
file_path = 'https://github.com/JakeOh/202505_BD50/raw/refs/heads/main/datasets/fish.csv'
```
```python
file = pd.read_csv(file_path)
file.head()
```
<img width="410" height="179" alt="image" src="https://github.com/user-attachments/assets/860fb8aa-2a31-45a0-85b6-f49e15ebdaac" />

```python
df = fish[fish.Species.isin(['Bream', 'Smelt'])]
```
```python
df.Species.unique()
```
<img width="282" height="35" alt="image" src="https://github.com/user-attachments/assets/38d3a0bb-9c48-439f-8608-46804386b644" />

```python
np.unique(df.Species, return_counts = True)    # Bream은 35, Smelt는 14마리
```
<img width="415" height="28" alt="image" src="https://github.com/user-attachments/assets/c5f4ccf3-0da3-4a90-b7cf-c865599ae9cc" />

```python
X = df.iloc[:, 1:].values    # 특성 배열
y = df.Species.values        # 타겟 배열
```
```python
X[:5, :]
```
<img width="431" height="92" alt="image" src="https://github.com/user-attachments/assets/0acd1799-cf98-4435-822c-70609881dfec" />

```python
y[:5]
```
<img width="481" height="31" alt="image" src="https://github.com/user-attachments/assets/9fd65d05-dcde-44aa-a236-2f58cc1ad45b" />

```python
X[-5:, :]
```
<img width="394" height="95" alt="image" src="https://github.com/user-attachments/assets/2f248a44-e04d-4379-95d6-7e06fc3e45b8" />

```python
y[-5:]
```
<img width="472" height="33" alt="image" src="https://github.com/user-attachments/assets/53980f15-a26b-47f7-af80-1647590d6fa2" />

# 훈련 셋/테스트 셋 나누기

```python
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.25,
                                                    random_state = 42,
                                                    stratify = y)  # stratify = y -> y의 클래스 비율을 그대로 유지한 채 분할
```
```python
X_train.shape   # (samples, features)
```
<img width="63" height="24" alt="image" src="https://github.com/user-attachments/assets/76d972de-52f5-4093-afd1-dfba08e2f8f2" />

```python
X_test.shape  
```
<img width="62" height="27" alt="image" src="https://github.com/user-attachments/assets/46dba641-9a7c-499f-8a10-fbbb5d2df030" />

```python
y_train.shape   # (samples,)
```
<img width="49" height="30" alt="image" src="https://github.com/user-attachments/assets/1ff8d7c9-ce48-4b99-8c74-32d6e5e09c40" />

```python
y_test.shape
```
<img width="57" height="30" alt="image" src="https://github.com/user-attachments/assets/e72a8e94-07e3-4b4e-943c-688ab4c938d5" />

```python
np.unique(y_train, return_counts = True)
```
<img width="427" height="28" alt="image" src="https://github.com/user-attachments/assets/79c97feb-7d93-4787-90ad-b479cfc08546" />

```python
np.unique(y_test, return_counts=True)
```
<img width="403" height="25" alt="image" src="https://github.com/user-attachments/assets/97183c55-2191-4a92-bebb-bff5153abcad" />

# KNN Classifier

```python
knn_pipe = Pipeline(steps = [('scaler', StandardScaler()),
                             ('clf', KNeighborsClassifier())])
```
```python
knn_pipe.fit(X_train, y_train)
```
<img width="251" height="141" alt="image" src="https://github.com/user-attachments/assets/d7227662-1a36-49d8-9710-c041bc02eff2" />

```python
# 훈련 셋에서의 예측값
train_pred = knn_pipe.predict(X_train)
```
```python
train_pred[:5]
```
<img width="476" height="31" alt="image" src="https://github.com/user-attachments/assets/40fe15cd-1748-40f6-9a83-3ee11194bab6" />

```python
y_train[:5]     # 실젯값(실제 레이블)
```
<img width="475" height="29" alt="image" src="https://github.com/user-attachments/assets/e9ed07a4-13f2-4d53-bf31-ea786d03716d" />

```python
confusion_matrix(y_train, train_pred)  # 정확도 100%
```
<img width="128" height="38" alt="image" src="https://github.com/user-attachments/assets/75e1713b-66a6-4f2e-a7f8-1092d39f6bca" />

```python
# 훈련 셋에서의 예측 확률
train_pred_prob = knn_pipe.predict_proba(X_train)
```
```python
train_pred_prob.shape
```
<img width="62" height="26" alt="image" src="https://github.com/user-attachments/assets/5490dd5c-fb3c-4511-8eae-b94c9aaca771" />

```python
train_pred_prob[:5]
```
<img width="131" height="86" alt="image" src="https://github.com/user-attachments/assets/fbb5b2ae-f4cd-4b19-93d0-a3ede2347fb2" />

```python
# 테스트 셋에서의 예측 확률
test_pred_prob = knn_pipe.predict_proba(X_test)
```
```python
test_pred_prob
```
<img width="141" height="212" alt="image" src="https://github.com/user-attachments/assets/dbf940b6-4164-499b-8959-773efd0b20cb" />

```python
# 테스트 셋에서의 예측값
test_pred = knn_pipe.predict(X_test)
```
```python
confusion_matrix(y_test, test_pred)
```
<img width="112" height="42" alt="image" src="https://github.com/user-attachments/assets/1b360c03-c149-4f19-bf2f-f7b3081dfa56" />

# LogisticRegression

```
# 회귀를 다루는게 아니라 분류를 다루는거!
회귀는 숫자를 예측하는 문제(집값, 온도 등)고,
분류는 항목을 맞히는 문제(고양이 vs 개)
```

```python
log_reg = Pipeline(steps = [('scaler', StandardScaler()),
                            ('clf', LogisticRegression(random_state=42))])
```
```python
log_reg.fit(X_train, y_train)
```
<img width="232" height="136" alt="image" src="https://github.com/user-attachments/assets/63999e0e-b619-450c-b7b8-bfa8e86a5c74" />

```python
# 훈련 셋에서의 각 클래스(Bream/Smelt)의 예측 확률
train_pred_prob = log_reg.predict_proba(X_train)
```
```python
train_pred_prob[:5, :]
```
<img width="244" height="88" alt="image" src="https://github.com/user-attachments/assets/d146a55a-ea45-4eef-b3ff-e8c4dc07ac2a" />

```python
# 훈련 셋에서의 예측값
train_pred = log_reg.predict(X_train)
```
```python
confusion_matrix(y_train, train_pred)
```
<img width="128" height="40" alt="image" src="https://github.com/user-attachments/assets/881cd701-0309-4a3c-aad6-ce11b24d7a10" />

```python
# 테스트 셋에서의 각 클래스 예측 확률
test_pred_prob = log_reg.predict_proba(X_test)
```
```python
test_pred_prob[:5, :]
```
<img width="300" height="94" alt="image" src="https://github.com/user-attachments/assets/1cb74abe-c984-49b5-b1c7-cc78011b50bd" />

```python
# 테스트 셋에서의 예측값
test_pred = log_reg.predict(X_test)
```
```python
confusion_matrix(y_test, test_pred)
```
<img width="118" height="42" alt="image" src="https://github.com/user-attachments/assets/055f0456-a7c8-48ea-998a-41e125d995a1" />

# Sigmoid(Logistic) 함수

$$
\phi(z) = \dfrac{e^z}{e^z + 1} = \dfrac{1}{1 + e^{-z}}
$$

```python
z = np.arange(-5, 5, 0.0001)    # x 좌표들
sigmoid = 1 / (1 + np.exp(-z))    # y 좌표들

plt.figure(figsize = (8, 4))
plt.plot(z, sigmoid)
plt.grid()
plt.xlabel('z')
plt.ylabel('$\phi(z)$')
plt.title('Sigmoid')
plt.show()
```
<img width="632" height="362" alt="image" src="https://github.com/user-attachments/assets/3a31992d-5a59-4266-8449-cce91dd5ceda" />

```
z값이 0일때 0.5
0 < sig < 1
sigmoid값이 0.5보다 크면 양성으로 예측, 0.5보다 작으면 음성으로 예측(이진 분류)
```

__Logistic Regression__

*   선형 회귀
$$
z_i = w_0 + x_{i1} \cdot w_1 + x_{i2} \cdot w_2 + \cdots = w_0 + \sum_j x_{ij} \cdot w_j
$$

*   선형 회귀 공식으로 계산된 결과를 logistic 함수의 argument로 전달
$$
p_i = \phi(z_i) = \dfrac{1}{1 + e^{-z}}
    = \dfrac{1}{1 + exp(-(w_0 + \sum_j x_{ij} \cdot w_j))}
$$

*   logistic 함수의 리턴값은 항상 0 ~ 1이므로, Logistic Regression은 logistic 함수의 리턴값을 양성이 될 확률로 해석.
*   Logistic Regression에서 훈련(학습)의 의미는 훈련 데이터로부터 아래의 비용 함수를 최소화하는 파라미터들($w_0$, $w_1$, ...)을 찾는 과정.


__비용(손실) 함수(cost/loss function)__

*   이진 분류에서는 손실 함수 log loss가 최소가 되는 계수들을 찾는 것이 목적.
*   Logistic loss function(__log loss__)


   $ L = [y* (-\log{p}) + (1-y) *(-\log{(1-p)})] $


   $ J(W) = - \dfrac{1}{N} \sum_{i=1}^{N} [y^{(i)} \log({p}^{(i)}) + (1-y^{(i)}) \log(1 - {p}^{(i)})] $


   N: the number of samples

*   다중 클래스 분류에서는 비용 함수 크로스 엔트로피가 최소가 되는 계수들을 찾는 것이 목적.
*   __Cross entropy__ cost function


   $ J(W) = - \dfrac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{m} y_{k}^{(i)} \log({p}_{k}^{(i)}) $


   N: the number of samples

   m: the number of classes

<img width="859" height="640" alt="image" src="https://github.com/user-attachments/assets/19853d6c-e95b-408d-a43d-5e1ec63487d1" />

```
선형 회귀 식을 sigmoid의 z자리에 넣는 것
Pi => i번째 샘플의 확률
예측이 맞으면 높은 점수를 주고, 틀리면 낮은 점수를 주는 것-> 비용 함수
```

```
Logistic Regression

선형 회귀 결과를 로지스틱 함수에 넣어서 출력값을 0~1 사이 확률로 변환해 주는 분류 모델

모델이 예측한 확률과 실제 정답(y) 사이의 오차를 측정하는 손실 함수를 최소화하도록 파라미터(w0, w1, ...)를 반복적으로 조정함
```

```python
p = np.arange(0.0001, 2.0, 0.0001)
y = np.log10(b)

plt.plot(p, y, label = '$log_{10}(p)$')
plt.plot(p, -y, label = '$-log_{10}(p)$')

plt.legend()
plt.grid()
plt.show()
```
<img width="493" height="383" alt="image" src="https://github.com/user-attachments/assets/21aa4c47-ce36-410b-97a6-9cbdbb4d23d0" />

```
# p: y = 1일 확률 -> 1-p: y = 0일 확률
# y(실제값) = 1 이면 L = −logp
# y = 0 이면 L = −log(1-p)

# p: 스팸일 확률 -> 0.1인데 스팸이면 손실이 커야함
# 1-p: 스팸이 아닐 확률 -> 0.9인데 스팸이 아니면 손실이 작아야함
# L=[y∗(−logp)+(1−y)∗(−log(1−p))] -> 물고기 한마리에 대한 손실값
```
