# LLM(Large Language Model, 거대 언어 모델)


*   시퀀스-시퀀스 작업(Sequence-to-Sequence)
    *   시퀀스 데이터를 입력받아서 시퀀스 데이터를 출력하는 작업
    *   자연어 처리(NLP, Natural Language Processing) 분야에서 요약, 번역 등의 작업 해당
    *   두 개의 (순환) 신경망을 연결한 인코더-디코더(encoder-decoder) 구조가 널리 사용
*   어텐션 메커니즘(Attention mechanism)
    *   인코더-디코더 구조에서 사용된 순환 신경망의 성능을 향상시키기 위해서 고안
    *   기존에는 인코더의 마지막 타임스텝에서 출력한 은닉 상태만을 사용해서 디코더가 새로운 텍스트를 생성
    *   어텐션 메커니즘은 모든 타임스텝에서 인코더가 출력한 은닉 상태를 디코더가 참조할 수 있도록 고안
    *   디코더가 새로운 토큰을 생성할 때 인코더가 처리한 토큰들 중에서 어떤 토큰에 주의(attention)를 기울일 지를 결정. (입력 토큰들마다 디코더가 중요도를 다르게 부여)
*   트랜스포머 모델(Transformer model)
    *   어텐션 메커니즘을 기반으로 하여 인코더-디코더 구조에서 순환층을 제거
    *   인코더에서 한 번에 하나의 토큰씩 처리하지 않고 입력 테스트 전체를 한 번에 처리
    *   핵심 구성 요소
        *   멀티 헤드 어텐션(multi-head attention)
        *   층 정규화(layer normalization)
        *   잔차 연결(residual connection)
        *   피드 포워드 네트워크(feed-forward network)
     
<img src="https://pbs.twimg.com/media/GJg3QtMXwAAvUT4?format=jpg&name=large"
    alt = "LLM 가계도" />


*   Encoder 기반 모델
    *   텍스트 (긍정/부정)분류
    *   개체명 인식 - 텍스트에서 사람 이름, 지역 이름, 회사 이름 등의 고유 명사를 식별.
    *   BERT, RoBEPTa, ...
*   Encoder-Decoder 기반 모델
    *   문서 요약, 변역, 질문-답변(질문과 문맥 텍스트가 주어졌을 때 문맥 안에서 답을 찾아서 생성)
    *   T5, BART, ...
*   Decoder 기반 모델
    *   텍스트 생성 - 챗봇, 질문 답변, 요약, 변역
    *   디코더: 
        *   이전까지 생성한 텍스트를 입력받아 다음 토큰을 예측하는 방식
        *   인코더로부터의 입력이 없으면 디코더는 아무것도 생성할 수 없음.
        *   이전에 생성한 텍스트인 것처럼 어떤 텍스트를 입력해 주면 인코더 도움 없이 다음 토큰을 예측.
        *   프롬프트(prompt): 이전에 생성한 텍스트인 것처럼 전달하는 초기 텍스트.
    *   GPT-4, GPT-5, LLaMA, ...
    *   현재 가장 활발히 연구되고 있는 LLM 분야.
 

# HuggingFace에서 KoBART 모델 사용하기

```python
import transformers     # HuggingFace를 사용하기 위한 패키지
```
```python
# HuggingFace에서 오픈 소스로 공개된 모델을 다운로드해서 로컬에서 실행할 수 있도록 해줌.
pipe = transformers.pipeline(task = "summarization",    # 문서 요약 작업
                             model = "sshleifer/distilbart-cnn-12-6")   # 모델    
# device = 0: GPU 사용, device = -1(기본값): CPU
```
<img width="1296" height="291" alt="image" src="https://github.com/user-attachments/assets/b7449a35-d7d7-4307-8fdb-360684810226" />

```python
type(pipe)  # Pipeline 클래스 객체
```
<img width="737" height="166" alt="image" src="https://github.com/user-attachments/assets/1a61bcbf-b8c1-46a8-a19a-5d7576ca6f41" />

```python
sample_test = \
'''
During the 1905–06 English football season, New Brompton F.C. competed in the Southern League Division One. It was the 12th season in which the club competed in the Southern League and the 11th in Division One. The team began the season in poor form; they failed to score any goals in six of their first eight Southern League games. By the midpoint of the season, the team had won only three times and were close to the bottom of the league table. The team's form improved in the new year, with three wins in the first seven Southern League games of 1906, but they ended the season in similar fashion to how they had started it, failing to score in eight of the final nine league games. New Brompton finished the season in 17th place out of 18 teams in the division.

New Brompton also competed in the FA Cup, reaching the second round. The team played a total of 37 league and cup matches, winning 8, drawing 9 and losing 20. Bill Marriott was the club's top goalscorer, with four goals in the Southern League and one in the FA Cup; this figure was the lowest to date with which a player had finished a season as New Brompton's top scorer. Joe Walton made the most appearances, playing in 36 of the team's 37 competitive games. The highest attendance recorded at the club's home ground, Priestfield Road, was 5,500 for a game against Portsmouth on 27 January 1906.

The club's first match of the season, on 2 September, was away to Queens Park Rangers; Floyd, Phillips, Sheridan and Marriott all made their Southern League debuts for New Brompton.[13] Queens Park Rangers scored two goals in each half to win 4–0; the correspondent for The Daily News stated that the home team were "the better side at every point" and well deserved their large victory.[14] The first Southern League game of the season at New Brompton's ground, Priestfield Road, a week later against Bristol Rovers, resulted in a 3–0 win for the away team.[13][15] Phillips scored New Brompton's first Southern League goal of the season on 16 September and the team achieved their first victory, beating Northampton Town 2–0,[13][16] before losing again in their next game, a 4–0 defeat away to Portsmouth.[17] After a goalless draw with Swindon Town, New Brompton were 15th out of 18 teams in the Division One table at the end of September.[18]
'''
```

```python
# Pipeline 객체를 함수처럼 호출할 수 있음.
result = pipe(sample_test)
```
```python
type(result)  # dict를 원소로 갖는 리스트(list)
```
<img width="44" height="26" alt="image" src="https://github.com/user-attachments/assets/c4bb53d1-663d-4535-849d-22f3d173129f" />

```python
# 리스트의 첫번째 원소 dict에서 키 summary_text에 해당하는 값을 출력
print(result[0]['summary_text'])   
```
```
During the 1905–06 English football season, New Brompton F.C. competed in the Southern League Division One . The team played 37 league and cup matches, winning 8, drawing 9 and losing 20 . The highest attendance recorded at the club's home ground, Priestfield Road, was 5,500 for a game against Portsmouth in January 1906 .
```
```python
sample_test_kor = \
'''
1896년 하계 올림픽(영어: 1896 Summer Olympics, Games of the I Olympiad, 그리스어: Θερινοί Ολυμπιακοί Αγώνες 1896)은 393년을 마지막으로 끝난 고대 올림픽 대회 이후 열린 첫 근대 올림픽 대회다. 1896년 4월 6일부터 4월 15일까지 그리스 아테네에서 개최되었다. 고대 그리스가 올림픽의 발상지여서 첫 근대 올림픽이 열리기에 적당한 장소였던 아테네는 1894년 6월 23일에 파리에서, 프랑스의 역사학자인 쿠베르탱이 주관한 올림픽 의회에서 만장일치로 개최지 자격을 얻었다. 또한 하계 올림픽이 진행되는 동안 국제 올림픽 위원회(IOC)가 조직되었다.

여러 어려움을 이겨낸 1회 올림픽은 성공적인 평가를 받았다. 당시의 국제 경기 중에서는 이 대회가 가장 많은 국제적인 참여를 이끌어냈다. 19세기 때 유일한 올림픽 경기장으로 쓰인 파나티나이코 경기장은 경기를 보려는 사람들이 몰려서 인산인해를 이루었다.[2] 개최국인 그리스에게 있어서 가장 절정이었던 부분은 마라톤 경기에서 자국 선수인 스피리돈 루이스가 승리의 영광을 차지한 것이었다. 이 대회에서는 카를 슈만이 레슬링과 체조에서 4개의 금메달을 따서 가장 많은 금메달을 획득했다. 가장 많이 메달을 딴 선수는 헤르만 바인게르트너로 6개의 메달을 땄다.

대회가 끝난 후 IOC는 이후의 올림픽을 계속 그리스에서 개최할 것인가를 놓고 피에르 드 쿠베르탱 남작과 그리스 임금인 요르요스 1세, 몇몇 미국 선수들 사이에서 갈등이 있었으나 1900년 대회가 이미 파리에서 열리기로 결정된 상태였고, 이후 올림픽은 세계를 순환하면서 개최하게 된다. 1906년 중간 올림픽을 제외하고는 그리스는 2004년 하계 올림픽을 개최할 때까지 108년간 올림픽을 개최하지 못했다.
'''
```

```
pipe(sample_text_kor)
해당 모델은 한글 텍스트로는 훈련이 되지 않아서 요약 작업을 실행할 수 없음.
```
```python
pipe_kor = transformers.pipeline(task = 'summarization',
                                 model = 'EbanLee/kobart-summary-v3')
```
<img width="684" height="222" alt="image" src="https://github.com/user-attachments/assets/19637d93-288b-41f2-9a1e-1d893955dc53" />

```python
pipe_kor(sample_test_kor)  # key 한개, value 한개
```
<img width="815" height="30" alt="image" src="https://github.com/user-attachments/assets/d9892ae7-8ec9-410a-a38f-e9d0370e6143" />

```python
# Pipeline 객체의 model 속성(property)의 config 속성(property)
print(pipe_kor.model.config)
```
```
BartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "author": "EbanLee(rudwo6769@gmail.com)",
  "bos_token_id": 1,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.1,
  "d_model": 768,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 1,
  "do_blenderbot_90_layernorm": false,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 1,
  "extra_pos_embeddings": 2,
  "force_bos_token_to_be_generated": false,
  "forced_eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "NEGATIVE",
    "1": "POSITIVE"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "kobart_version": 2.0,
  "label2id": {
    "NEGATIVE": 0,
    "POSITIVE": 1
  },
  "max_position_embeddings": 1026,
  "model_type": "bart",
  "normalize_before": false,
  "normalize_embedding": true,
  "num_hidden_layers": 6,
  "pad_token_id": 3,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 300,
      "min_length": 12,
      "no_repeat_ngram_size": 15,
      "num_beams": 6,
      "repetition_penalty": 1.5
    }
  },
  "tokenizer_class": "PreTrainedTokenizerFast",
  "torch_dtype": "float32",
  "transformers_version": "4.55.4",
  "use_cache": true,
  "vocab_size": 30000
}
```


```
토크나이저(tokenizer): 텍스트를 토큰(정수)으로 변환하는 객체.
```


```python
print(pipe_kor.tokenizer.vocab_size)    # tokenizer가 사용하는 어휘 사전의 크기
```
<img width="44" height="29" alt="image" src="https://github.com/user-attachments/assets/2b2451b8-56cc-4755-a971-7defe415de48" />



```
pipeline 함수는 LLM 모델과 텍스트를 토큰화하는 tokenizer 객체를 함께 다운로드함.

LLM모델이 문서 요약을 하기 전에 입력 텍스트를 tokenizer를 사용해서 전처리를 하고, 모델이 예측을 수행.
```


```python
vocab = pipe_kor.tokenizer.vocab  # 어휘 사전 - dict(단어를 키로 하고 토큰을 값으로 하는)
```
```python
list(vocab.items())[:10]

# 토큰 "나의"는 22128이라는 숫자로 인코딩
```

<img width="147" height="170" alt="image" src="https://github.com/user-attachments/assets/2ccbaa58-40f3-4fbd-948a-5d646d122da3" />

## tokenizer 메서드

```python
tokenizer = pipe_kor.tokenizer
```
```python
# tokenize(): 텍스트 --> 토큰들의 리스트
tokens = tokenizer.tokenize('혼자 공부하는 머신러닝 딥러닝')
```
```python
print(tokens)
```
<img width="549" height="27" alt="image" src="https://github.com/user-attachments/assets/1e713d68-6600-43e4-b51d-688da3af2f6b" />

```python
# convert_tokens_to_ids(): 토큰들의 리스트를 토큰에 해당하는 정수들의 리스트로 변환
ids = tokenizer.convert_tokens_to_ids(tokens)
```
```python
print(ids)
```
<img width="533" height="27" alt="image" src="https://github.com/user-attachments/assets/93ee8e33-f39a-49d0-ab01-8eed89f2a708" />

```python
# encode(): tokenize + convert_tokens_to_ids
encoded = tokenizer.encode('혼자 공부하는 머신러닝 딥러닝')
```
```python
print(encoded)  # 0은 문장 시작, 1은 문장 끝
```
<img width="568" height="26" alt="image" src="https://github.com/user-attachments/assets/a7a10ca0-dd69-4324-86df-5b2a58021a9b" />

```python
encoded2 = tokenizer.encode('KoBART 모델을 사용한 문서 요약')
print(encoded2)
```
<img width="476" height="27" alt="image" src="https://github.com/user-attachments/assets/7583d14e-3b1b-43a7-90a7-8c36e86064be" />

```python
# convert_ids_to_tokens(): 토큰 아이디들의 리스트를 토큰들의 리스트로 변환(디코딩 역할)
tokens2 = tokenizer.convert_ids_to_tokens(encoded2)
print(tokens2)
```
<img width="644" height="27" alt="image" src="https://github.com/user-attachments/assets/b3ad32c8-6f80-4340-9033-eb031ca4d42b" />

```python
# decode(): 토큰 아이디들을 갖는 리스트를 텍스트로 변환
tokenizer.decode(encoded2)

# <s> </s> 은 0과 1을 가지고 있어서
```
<img width="288" height="26" alt="image" src="https://github.com/user-attachments/assets/9220dde0-818d-4be5-bbb0-021d2321134a" />

```python
tokenizer.decode(encoded2[1:-1])
```
<img width="237" height="25" alt="image" src="https://github.com/user-attachments/assets/f93a9cdb-6279-47cc-b973-631193b0b3f1" />








Pipeline 동작 원리

텍스트 입력 --> tokenizer.encode() --> LLM 모델 --> tokenizer.decode()

