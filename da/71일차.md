# 활성화 함수(Activation Function)

*   활성화 함수: 신경망의 각 층(layer)에서 출력을 내보내는 함수.
*   신경망의 은닉층(hidden layer)에서 선형함수를 사용하면 여러 개의 층을 연결하는 효과가 없고, 단일 층을 사용하는 것과 같아짐.
*   은닉층에서는 비선형 함수를 활성화 함수로 사용.

## Sigmoid

sigmoid(x)=1 / +exp(−x)

```python
xvals = np.arange(-5, 5, 0.0001)    # -5부터 5까지 0.0001간격으로 배열 생성
```
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# 입력값을 0과 1사이의 값으로 변환
```
```python
yvals = sigmoid(xvals)
```
```python
plt.figure(figsize = (8, 4))
plt.plot(xvals, yvals)
plt.grid()
plt.show()
```
<img width="671" height="351" alt="image" src="https://github.com/user-attachments/assets/720ea70a-2aa2-4873-933f-8db6159202da" />

## Hyperbolic tangent

```python
def plot_activation_fn(fn):
    xvals = np.arange(-5, 5, 0.0001)
    yvals = fn(xvals)
    plt.figure(figsize = (8, 4))
    plt.plot(xvals, yvals)
    plt.grid()
    plt.show()
```
```python
plot_activation_fn(np.tanh)
```
<img width="692" height="351" alt="image" src="https://github.com/user-attachments/assets/f30ef326-80fa-4a79-9fb2-eab18452ca85" />

## ReLU(Rectified Linear Unit)

*   입력값이 양수이면 마치 활성화 함수가 없는 것처럼 입력값 그대로 출력으로 내보냄.
*   입력값이 음수이면 출력값을 0으로 만듦.
*   이미지 처리를 할 때 좋은 성능을 낸다고 알려져 있음.

<img width="230" height="60" alt="image" src="https://github.com/user-attachments/assets/ca1c207d-062d-4ad5-b652-b0e939b0225d" />

```python
def relu(x):
    return np.maximum(x, 0)     # 더 큰 값 반환
```
```python
plot_activation_fn(relu)
```
<img width="658" height="351" alt="image" src="https://github.com/user-attachments/assets/20201122-47c8-4ad9-b56c-4f7521f18e09" />

## Leaky ReLU

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/64f77c7f-1cf2-4bc0-b98f-4152ef4eb6e7" />

```python
def leaky_relu(x, alpha = 0.1):
    # 0 <= alpha <= 1
    return np.maximum(x, alpha * x)
```
```python
plot_activation_fn(leaky_relu)
```
<img width="658" height="351" alt="image" src="https://github.com/user-attachments/assets/c571e2b6-eaf4-4ab7-a4d0-d89e3b9a872e" />

## ELU(Exponential Linear Unit)

<img width="331" height="68" alt="image" src="https://github.com/user-attachments/assets/46e6941a-2680-4b0b-b6bc-6c5dd915b8a4" />

```python
def elu(arr, alpha = 1):
    # 0 <= alpha
    result = []
    for x in arr:
        if x >= 0:
            result.append(x)
        else:
            result.append(alpha * (np.exp(x) - 1))
    return np.array(result)
```
```python
def elu(x, alpha = 1):
    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))
```
```python
plot_activation_fn(elu)
```
<img width="670" height="351" alt="image" src="https://github.com/user-attachments/assets/ee7154f3-98ac-4342-ae03-5eafc0055dca" />

```
은닉층에서의 활성화 함수 선택: ELU > LeakyReLU > ReLU > tanh > sigmoid
```

## ReLU를 활성화 함수로 사용하는 은닉층

```python
model = keras.Sequential()
```
```python
model.add(keras.Input(shape = (28, 28)))   # 입력층
```
```python
model.add(keras.layers.Flatten())   # (28, 28) shape의 배열을 (784,) shape으로 변환
```
```python
# 100개 unit을 갖는, ReLU를 활성화 함수로 사용하는 은닉층을 추가
model.add(keras.layers.Dense(units = 100, activation = 'relu'))
```
```python
# 출력층 - 다중 클래스 분류. 10개의 unit을 갖고 softmax를 활성화 함수로 사용하는 층.
model.add(keras.layers.Dense(units = 10, activation = 'softmax'))
```
```python
model.summary()
```
<img width="547" height="206" alt="image" src="https://github.com/user-attachments/assets/372b6b20-683c-4477-b5d2-430c23ca0509" />

```python
# 모델 컴파일
model.compile(loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
# 모델 훈련
model.fit(x = train_data_scaled, y = train_target, epochs = 5)
```
<img width="836" height="177" alt="image" src="https://github.com/user-attachments/assets/6ea11c9e-55bb-4c96-a97a-22fcc4527ec1" />

```python
# 모델 평가 - 훈련 셋의 손실/정확도
model.evaluate(x = train_data_scaled, y = train_target)
```
<img width="833" height="41" alt="image" src="https://github.com/user-attachments/assets/4db91bcd-413c-476f-914c-a3caa1b7bff2" />

```python
# 모델 평가 - 검증 셋의 손실/정확도
model.evaluate(x = val_data_scaled, y = val_target)
```
<img width="822" height="39" alt="image" src="https://github.com/user-attachments/assets/4db8c10e-b313-41ce-93dd-d38ac151d3af" />

## ELU를 활성화 함수로 사용하는 은닉층

```python
model = keras.Sequential()
model.add(keras.Input(shape = (28, 28)))   # 입력층
model.add(keras.layers.Flatten())   # Flatten layer: 2차원 입력 배열 -> 1차원 배열
model.add(keras.layers.Dense(units = 100, activation = 'elu'))  # 은닉층(hidden layer)
model.add(keras.layers.Dense(units = 10, activation = 'softmax'))   # 출력층
```
```python
model.summary()
```
<img width="548" height="205" alt="image" src="https://github.com/user-attachments/assets/3722b418-7403-4284-8811-494b9ccd2de5" />

```python
model.compile(loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
model.fit(x = train_data_scaled, y = train_target, epochs = 5)
```
<img width="834" height="183" alt="image" src="https://github.com/user-attachments/assets/8162fea3-07e5-413b-a1cc-e49de0f4ee55" />

```python
model.evaluate(x = train_data_scaled, y = train_target)
```
<img width="835" height="42" alt="image" src="https://github.com/user-attachments/assets/e0b170fb-d9a4-495f-a259-ecda0fa8ce75" />

```python
model.evaluate(x = val_data_scaled, y = val_target)
```
<img width="820" height="41" alt="image" src="https://github.com/user-attachments/assets/2a63cbab-ecff-45cd-8165-cf1680e6a016" />

# Optimizer

*   경사 하강법(Gradient Descent)
    *   손실 함수를 최소로 만드는 가중치들을 찾는 방법.


<img width="226" height="35" alt="image" src="https://github.com/user-attachments/assets/494a0d88-43fc-49e7-8d45-a01b4d879bee" />


*   아주 큰 신경망인 경우는 모델 훈련을 통해서 찾아야 하는 모델 파라미터들의 개수가 매우 많음.
*   모델 파라미터들의 개수가 많을 수록 훈련 속도가 심각하게 느려질 수 있음.
*   훈련 속도를 높이는 방법:
    *   좋은 초기화 전략 사용하기
    *   좋은 활성화 함수 사용하기
    *   배치 정규화 사용하기
    *   사전 훈련된 네트워크(신경망)의 일부를 재사용.
    *   일반적인 경사 하강법 대신 더 빠른 옵티마이저를 사용하기.
*   확률적 경사 하강법(SGD)의 변종 옵티마이저
    *   학습률(learning_rate) 파라미터를 변경.
    *   모멘텀 최적화(momentum optimization): 모멘텀(momentum) 파라미터 값을 0보다 큰 값으로 변경.
    *   네스테로프 가속 경사(Nesterov Accelerated Gradient, NAG): nesterov 파라미터를 True로 설정.
*   적응형 학습률(adaptive learning rate): 학습할 때마다 학습률을 변화시킴.
    *   RMSprop(Root Mean Squared Propagation): Sequential 클래스의 compile 메서드의 optimizer 기본값.
    *   Adam(Adaptive Momentum Estimation): 모멘텀 최적화 + RMSprop
    *   Nadam: Adam + Nesterov
    *   AdaGrad
 
## SGD optimizer

```python
def create_model(layers = None):
    # layers: keras.layers 객체들의 배열/리스트.
    model = keras.Sequential()  # Seqential 모델 생성
    model.add(keras.Input(shape = (28, 28)))    # 입력층 추가
    model.add(keras.layers.Flatten())   # Flatten layer 추가
    model.add(keras.layers.Dense(units = 100, activation = 'relu'))     # 첫번째 은닉층을 추가
    if layers:  # 아규먼트로 전달된 은닉층들이 있으면
        for layer in layers:
            model.add(layer)
    model.add(keras.layers.Dense(units = 10, activation = 'softmax'))   # 출력층 추가

    return model
```
```python
model = create_model()
```
```python
model.summary()
```
<img width="561" height="204" alt="image" src="https://github.com/user-attachments/assets/13d9859f-3011-4772-9064-7b238ded3753" />

```python
# 옵티마이저: 학습률 변경, 모멘텀 최적화, 네스테로프 가속 경사
op = keras.optimizers.SGD(learning_rate = 0.02, momentum = 0.1, nesterov = True)
model.compile(optimizer = op,
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
model.fit(x = train_data_scaled, y = train_target, epochs = 5)
```
<img width="853" height="187" alt="image" src="https://github.com/user-attachments/assets/972de537-f939-40f1-b3a1-f1cafd3766eb" />

```python
model.evaluate(x = train_data_scaled, y = train_target)
```
<img width="830" height="43" alt="image" src="https://github.com/user-attachments/assets/b845611b-4436-4468-85ab-a0972e9089d6" />

```python
model.evaluate(x = val_data_scaled, y = val_target)
```
<img width="825" height="42" alt="image" src="https://github.com/user-attachments/assets/1c083a2d-17e2-40f5-ad81-c1f939761871" />

## Adam optimizer

*   `learning_rate` 파라미터: 학습률
*   `beta_1`, `beta_2` 파라미터: 모멘텀 최적화와 관련된 파라미터.
*   `epsilon` 파라미터: 적응형 학습률과 관련된 파라미터.

```python
model = create_model()
```
```python
model.compile(optimizer = keras.optimizers.Adam(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
model.fit(x = train_data_scaled, y = train_target, epochs = 5)
```
<img width="843" height="179" alt="image" src="https://github.com/user-attachments/assets/efe16b65-5d6e-43b4-a387-fc11779b8258" />

```python
model.evaluate(x = train_data_scaled, y = train_target)
```
<img width="842" height="45" alt="image" src="https://github.com/user-attachments/assets/04302482-d6dd-4010-974f-f3daa40b1725" />

```python
model.evaluate(x = val_data_scaled, y = val_target)
```
<img width="816" height="41" alt="image" src="https://github.com/user-attachments/assets/3a91321e-095f-4c78-a9ae-d3ad93a01129" />

# 모델 훈련

```python
model = create_model()  # 은닉층 1개를 갖는 모델을 생성
model.compile(optimizer = keras.optimizers.RMSprop(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])    # 모델 컴파일
```
```python
result = model.fit(x = train_data_scaled, y = train_target, epochs = 5,
                   validation_data = [val_data_scaled, val_target])
```
<img width="1263" height="169" alt="image" src="https://github.com/user-attachments/assets/9c9c78ef-2ae5-4577-833e-83f7028b6ecf" />

## History 객체

신경망 모델을 훈련(fit)시키는 과정(각각의 epoch)의 기록을 담고 있는 객체. fit 메서드의 리턴 값.

```python
type(result)
```
<img width="740" height="163" alt="image" src="https://github.com/user-attachments/assets/239318c8-f568-4fd8-b33e-35bf95fc52d6" />

```python
type(result.history)
```
<img width="45" height="25" alt="image" src="https://github.com/user-attachments/assets/61b71b72-41bc-43bb-b2a1-c03f5b8e5cf7" />

```python
hist = result.history
# 각 epoch마다 훈련 셋 손실/정확도, 검증 셋 손실/정확도를 dict로 저장.
```
```python
hist.keys()
```
<img width="450" height="43" alt="image" src="https://github.com/user-attachments/assets/8cfc2987-0e81-4d78-8c70-a68e62a2af16" />

```python
hist.items()
```
<img width="1255" height="57" alt="image" src="https://github.com/user-attachments/assets/a332df6d-a100-466b-9eb5-232f059e64ab" />

```python
epochs = np.arange(1, 6)
for k, v in hist.items():
    plt.plot(epochs, v, 'o-', label = k)
plt.legend()
plt.xlabel('epoch')
plt.show()
```
<img width="547" height="432" alt="image" src="https://github.com/user-attachments/assets/279cb449-9bbb-459b-883e-807c2e8497c7" />

```python
# 손실 그래프
plt.plot(epochs, hist['loss'], 'bo-', label = 'train')
plt.plot(epochs, hist['val_loss'], 'ro-', label = 'validation')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()
```
<img width="575" height="432" alt="image" src="https://github.com/user-attachments/assets/b8798db6-e6c7-4738-93c6-7c17f4801e9c" />

## RMSprop 옵티마이저 사용

*   100개의 unit으로 이루어진 은닉층 1개를 갖는 신경망 모델 생성
*   RMSprop 옵티마이저를 사용하도록 모델을 컴파일
*   epoch 횟수를 20회로 모델을 훈련시키고 그 결과를 저장.
*   History 객체의 history 속성을 사용해서 훈련 셋과 검증 셋의 손실 그래프를 그려 보세요.

```python
model = create_model()
model.compile(optimizer = keras.optimizers.RMSprop(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
result = model.fit(x = train_data_scaled, y = train_target, epochs = 20,
                   validation_data = [val_data_scaled, val_target])
```
```
Epoch 1/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.6834 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.3954 - val_sparse_categorical_accuracy: 0.8586
Epoch 2/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 9s 3ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.8583 - val_loss: 0.3706 - val_sparse_categorical_accuracy: 0.8691
Epoch 3/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8729 - val_loss: 0.3636 - val_sparse_categorical_accuracy: 0.8711
Epoch 4/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.8800 - val_loss: 0.3727 - val_sparse_categorical_accuracy: 0.8719
Epoch 5/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 11s 4ms/step - loss: 0.3177 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8798
Epoch 6/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3055 - sparse_categorical_accuracy: 0.8903 - val_loss: 0.3306 - val_sparse_categorical_accuracy: 0.8867
Epoch 7/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 4ms/step - loss: 0.2949 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.3469 - val_sparse_categorical_accuracy: 0.8827
Epoch 8/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2823 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.8827
Epoch 9/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2785 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8871
Epoch 10/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2716 - sparse_categorical_accuracy: 0.9044 - val_loss: 0.3520 - val_sparse_categorical_accuracy: 0.8864
Epoch 11/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 4ms/step - loss: 0.2638 - sparse_categorical_accuracy: 0.9066 - val_loss: 0.3547 - val_sparse_categorical_accuracy: 0.8780
Epoch 12/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9080 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8867
Epoch 13/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2502 - sparse_categorical_accuracy: 0.9134 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8841
Epoch 14/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 11s 4ms/step - loss: 0.2457 - sparse_categorical_accuracy: 0.9121 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8878
Epoch 15/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - loss: 0.2466 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.3803 - val_sparse_categorical_accuracy: 0.8854
Epoch 16/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - loss: 0.2383 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.3940 - val_sparse_categorical_accuracy: 0.8812
Epoch 17/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 7s 5ms/step - loss: 0.2285 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.4045 - val_sparse_categorical_accuracy: 0.8832
Epoch 18/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 4ms/step - loss: 0.2321 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.3832 - val_sparse_categorical_accuracy: 0.8867
Epoch 19/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 9s 3ms/step - loss: 0.2291 - sparse_categorical_accuracy: 0.9197 - val_loss: 0.3943 - val_sparse_categorical_accuracy: 0.8850
Epoch 20/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.2246 - sparse_categorical_accuracy: 0.9207 - val_loss: 0.3798 - val_sparse_categorical_accuracy: 0.8915
```
```python
epochs = np.arange(1, 21)
```
```python
history = result.history
plt.plot(epochs, history['loss'], 'bo-', label = 'train')
plt.plot(epochs, history['val_loss'], 'ro:', label = 'validation')
plt.grid()
plt.legend()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```
<img width="575" height="432" alt="image" src="https://github.com/user-attachments/assets/265c3e68-fff0-4c0d-9e15-09a63b5eb146" />

## Adam 옵티마이저 사용

*   위의 과정을 반복


```python
model = create_model()
model.compile(optimizer = keras.optimizers.Adam(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
result = model.fit(x = train_data_scaled, y = train_target, epochs = 20,
                   validation_data = [val_data_scaled, val_target])
```
```
Epoch 1/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - loss: 0.6808 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4410 - val_sparse_categorical_accuracy: 0.8483
Epoch 2/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.4092 - sparse_categorical_accuracy: 0.8547 - val_loss: 0.3918 - val_sparse_categorical_accuracy: 0.8622
Epoch 3/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.3550 - sparse_categorical_accuracy: 0.8733 - val_loss: 0.3696 - val_sparse_categorical_accuracy: 0.8660
Epoch 4/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 11s 4ms/step - loss: 0.3330 - sparse_categorical_accuracy: 0.8789 - val_loss: 0.3311 - val_sparse_categorical_accuracy: 0.8820
Epoch 5/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 4ms/step - loss: 0.3040 - sparse_categorical_accuracy: 0.8906 - val_loss: 0.3359 - val_sparse_categorical_accuracy: 0.8811
Epoch 6/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2903 - sparse_categorical_accuracy: 0.8929 - val_loss: 0.3387 - val_sparse_categorical_accuracy: 0.8809
Epoch 7/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 7s 5ms/step - loss: 0.2802 - sparse_categorical_accuracy: 0.8950 - val_loss: 0.3227 - val_sparse_categorical_accuracy: 0.8838
Epoch 8/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2674 - sparse_categorical_accuracy: 0.9007 - val_loss: 0.3292 - val_sparse_categorical_accuracy: 0.8834
Epoch 9/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2552 - sparse_categorical_accuracy: 0.9054 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8891
Epoch 10/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.2469 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.3162 - val_sparse_categorical_accuracy: 0.8888
Epoch 11/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.2340 - sparse_categorical_accuracy: 0.9121 - val_loss: 0.3144 - val_sparse_categorical_accuracy: 0.8898
Epoch 12/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2308 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.3262 - val_sparse_categorical_accuracy: 0.8884
Epoch 13/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 11s 4ms/step - loss: 0.2214 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.3312 - val_sparse_categorical_accuracy: 0.8857
Epoch 14/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2221 - sparse_categorical_accuracy: 0.9174 - val_loss: 0.3150 - val_sparse_categorical_accuracy: 0.8929
Epoch 15/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.2106 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.8863
Epoch 16/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2064 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.8897
Epoch 17/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.2026 - sparse_categorical_accuracy: 0.9232 - val_loss: 0.3258 - val_sparse_categorical_accuracy: 0.8940
Epoch 18/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.1977 - sparse_categorical_accuracy: 0.9267 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8910
Epoch 19/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9278 - val_loss: 0.3349 - val_sparse_categorical_accuracy: 0.8890
Epoch 20/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.1854 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.3359 - val_sparse_categorical_accuracy: 0.8940
```
```python
def plot_train_val_losses(history):
    plt.plot(history.epoch, history.history['loss'], 'bo-', label='train')
    plt.plot(history.epoch, history.history['val_loss'], 'ro:', label='validation')
    plt.grid()
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.show()
```
```python
plot_train_val_losses(result)
```
<img width="575" height="432" alt="image" src="https://github.com/user-attachments/assets/61be647c-9eb4-426c-b4b0-a1611c4b8975" />

```
epoch를 6번 이상 수행하더라도 검증 셋의 손실이 더 이상 작아지지 않음.
```

# Dropout

*   훈련 과정(epoch)에서 일부 unit의 출력을 0으로 만드는 것(그 다음 layer로 출력값을 전달하지 않음).
*   각 epoch마다 출력을 0으로 만드는 unit은 무작위로 선택.
*   모델의 과대적합을 줄일 수 있음.

```python
model = create_model(layers = [keras.layers.Dropout(rate = 0.3)])
# 은닉층 뒤에 30% 비율로 출력을 0으로 만드는 Dropout 계층을 추가
```
```python
model.summary()
```
<img width="558" height="238" alt="image" src="https://github.com/user-attachments/assets/4e49e6f5-4427-45f0-b517-b062e9cfe4ee" />

```python
model.compile(optimizer = keras.optimizers.Adam(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
result = model.fit(x = train_data_scaled, y = train_target, epochs = 20,
                   validation_data = [val_data_scaled, val_target])
```
```
Epoch 1/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - loss: 0.7736 - sparse_categorical_accuracy: 0.7310 - val_loss: 0.4206 - val_sparse_categorical_accuracy: 0.8502
Epoch 2/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 9s 3ms/step - loss: 0.4518 - sparse_categorical_accuracy: 0.8373 - val_loss: 0.3751 - val_sparse_categorical_accuracy: 0.8647
Epoch 3/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.4131 - sparse_categorical_accuracy: 0.8505 - val_loss: 0.3624 - val_sparse_categorical_accuracy: 0.8689
Epoch 4/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 4ms/step - loss: 0.3916 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.3534 - val_sparse_categorical_accuracy: 0.8702
Epoch 5/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - loss: 0.3641 - sparse_categorical_accuracy: 0.8635 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.8775
Epoch 6/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.3595 - sparse_categorical_accuracy: 0.8685 - val_loss: 0.3360 - val_sparse_categorical_accuracy: 0.8760
Epoch 7/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.8731 - val_loss: 0.3354 - val_sparse_categorical_accuracy: 0.8776
Epoch 8/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3391 - sparse_categorical_accuracy: 0.8743 - val_loss: 0.3220 - val_sparse_categorical_accuracy: 0.8809
Epoch 9/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3270 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.3398 - val_sparse_categorical_accuracy: 0.8788
Epoch 10/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.3173 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.3249 - val_sparse_categorical_accuracy: 0.8813
Epoch 11/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3133 - sparse_categorical_accuracy: 0.8835 - val_loss: 0.3324 - val_sparse_categorical_accuracy: 0.8806
Epoch 12/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3161 - sparse_categorical_accuracy: 0.8816 - val_loss: 0.3179 - val_sparse_categorical_accuracy: 0.8889
Epoch 13/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.3017 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.3123 - val_sparse_categorical_accuracy: 0.8895
Epoch 14/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.3178 - val_sparse_categorical_accuracy: 0.8867
Epoch 15/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.2996 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.3200 - val_sparse_categorical_accuracy: 0.8873
Epoch 16/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2923 - sparse_categorical_accuracy: 0.8895 - val_loss: 0.3142 - val_sparse_categorical_accuracy: 0.8907
Epoch 17/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2894 - sparse_categorical_accuracy: 0.8912 - val_loss: 0.3221 - val_sparse_categorical_accuracy: 0.8855
Epoch 18/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.2825 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.3161 - val_sparse_categorical_accuracy: 0.8906
Epoch 19/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 0.2848 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.3102 - val_sparse_categorical_accuracy: 0.8893
Epoch 20/20
1500/1500 ━━━━━━━━━━━━━━━━━━━━ 5s 3ms/step - loss: 0.2741 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.3213 - val_sparse_categorical_accuracy: 0.8892
```
```python
plot_train_val_losses(result)
```
<img width="576" height="432" alt="image" src="https://github.com/user-attachments/assets/1dbfd055-4dd0-4c0a-bf40-935eb6a92b95" />
