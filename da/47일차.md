# Decision Tree

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report
```

## 데이터 셋

```python
# wine 데이터셋
wine_csv = 'https://bit.ly/wine_csv_data'
```
```python
wine_df = pd.read_csv(wind_csv)
```
```python
wine_df.head()
```
<img width="243" height="176" alt="image" src="https://github.com/user-attachments/assets/24d9f8ac-c77d-418d-bb2d-3fb2e53f3a99" />

```python
wine_df.info()
```
<img width="282" height="181" alt="image" src="https://github.com/user-attachments/assets/91264171-4be5-4208-9902-23cff590147e" />

```python
wine_df.describe()
```
<img width="401" height="256" alt="image" src="https://github.com/user-attachments/assets/53d0dcfe-dc43-4c10-bedc-d32bc20f686e" />

```python
class_counts = wine_df['class'].value_counts()
```
```python
class_counts   # 0, 1만 있는 카테고리(0 - red wine, 1 - white wine)
```
<img width="126" height="157" alt="image" src="https://github.com/user-attachments/assets/77371d27-dcf7-44d4-b125-0f908e9f5d2e" />

```python
wind_df.columns[0:3]
```
<img width="361" height="33" alt="image" src="https://github.com/user-attachments/assets/6e01316e-daa8-4ecb-aea2-74270a3ffec6" />

## alcohol, sugar, pH 변수들의 히스토그램

```python
fig, axes = plt.subplots(ncols = 3, figsize = (16, 4))

for i, col in enumerate(wine_df.columns[:3]):
    sns.hisplot(data = wine_df, x = col, ax = axes[i])
plt.show()
```
<img width="844" height="240" alt="image" src="https://github.com/user-attachments/assets/b1e5917b-30cc-4572-8a7d-a072764388a9" />

## class 별로 색깔을 다르게 시각화한 pairplot

```python
sns.pairplot(data = wine_df, hue = 'class')
plt.show()
```
<img width="771" height="669" alt="image" src="https://github.com/user-attachments/assets/3e0531b0-65d4-49ce-a233-796a47bdf84f" />

## 훈련/테스트 나누기

```python
X = wind_df[wine_df.columns[:3]].values    # 특성 배열(alcohol, sugar, pH)
y = wine_df['class'].values    # 타겟 배열(class)
```
```python
X[:5, :]
```
<img width="204" height="86" alt="image" src="https://github.com/user-attachments/assets/33b0799c-28e9-4932-82fb-4090a763a985" />

```python
y[:5]
```
<img width="205" height="28" alt="image" src="https://github.com/user-attachments/assets/2ab8f472-7711-4fac-8894-391536316336" />

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)
```
```python
X_train.shape
```
<img width="80" height="32" alt="image" src="https://github.com/user-attachments/assets/6b28f291-6df9-4c8a-ad65-5596fba14e59" />

```python
X_test.shape
```
<img width="79" height="32" alt="image" src="https://github.com/user-attachments/assets/2efdb729-d55b-40a1-8bab-9b166be3eb03" />

```python
y_train.shape
```
<img width="66" height="30" alt="image" src="https://github.com/user-attachments/assets/e8f69f82-a485-430a-83f5-97d644e08f5f" />

```python
y_test.shape
```
<img width="66" height="31" alt="image" src="https://github.com/user-attachments/assets/41840277-9c94-462c-a749-4d80bc04bc5d" />

```python
feature_names = wine_df.columns[:3]    # 특성 이름
target_names = ['red', 'white']    # 클래스 이름(레이블)
print(feature_names)
print(target_names)
```
<img width="354" height="43" alt="image" src="https://github.com/user-attachments/assets/978cdb25-3551-4dc0-8ff7-12609506af6f" />

# Decision Tree

```python
tree_clf = DecisionTreeClassifier(random_state = 42)    # 모델 생성
```
```python
tree_clf.fit(X_train, y_train)    # 모델 훈련
```
<img width="308" height="71" alt="image" src="https://github.com/user-attachments/assets/5c8d458f-c4e7-4587-9e39-6dc82741d678" />

```python
train_pred = tree_clf.predict(X_train)    # 훈련 셋 예측값
```
```python
cm_train = confusion_matrix(y_train, train_pred)
cm_train
```
<img width="158" height="46" alt="image" src="https://github.com/user-attachments/assets/71115176-6239-4f50-8287-2f176a8b732e" />

```python
sns.heatmap(data = cm_train, cmap = 'Blues', annot = True, fmt = 'd',
            xticklabels = target_names, yticklabels = target_names)
plt.show()
```
<img width="477" height="379" alt="image" src="https://github.com/user-attachments/assets/dc2ab864-dbe6-4198-978d-9aa3bc2eccce" />

```python
print(classification_report(y_train, train_pred))
```
<img width="396" height="146" alt="image" src="https://github.com/user-attachments/assets/214c3ae4-6559-4880-9340-8f671bc2fde6" />

```python
test_pred = tree_clf.predict(X_test)    # 테스트 셋 예측값
```
```python
cm_test = confusion_matrix(y_test, test_pred)
cm_test
```
<img width="149" height="44" alt="image" src="https://github.com/user-attachments/assets/19bffbe3-53e9-4395-8153-67214f64a8b7" />

```python
sns.heatmap(data = cm_test, cmap = 'Blues', annot = True, fmt = 'd',
            xticklabels = target_names, yticklabels = target_names)
plt.show()
```
<img width="479" height="371" alt="image" src="https://github.com/user-attachments/assets/a8b4e930-1b9a-4114-a94b-1ce9d74a004a" />

```python
print(classification_report(y_test, test_pred))
```
<img width="400" height="135" alt="image" src="https://github.com/user-attachments/assets/72a4315c-7a35-4958-a933-2bcf2616ff12" />


현재까지 훈련된 Decision Tree는 과대적합이 매우 크다.


```python
plot_tree(tree_clf)
plt.show()
```
<img width="484" height="350" alt="image" src="https://github.com/user-attachments/assets/df063a68-df55-4ded-b6c3-aba05ecb54e6" />

```python
plt.figure(figsize = (16, 12))
plot_tree(tree_clf, max_depth = 2, feature_names = feature_names, class_names = target_names, filled = True)
# feature_names 특성 이름 무엇으로 할거냐. filled : 노드들의 색을 채울거냐
plt.show()
```
<img width="844" height="622" alt="image" src="https://github.com/user-attachments/assets/4a23bc9f-a8b5-41e3-8d05-27d559f4985b" />

```
# 레드가 많으면 많을 수록 오렌지 색이 진해짐, 화이트가 많을 수록 파란색으로 됨
# 당도가 4.325보다 작거나 같으면 왼쪽. 
# samples은 총 수, value = [1199, 1747] (레드 수, 화이트 수) 두개의 수 중에 많은걸로 class 정함. 
# 레드와 화이트 수가 차이 많이 안나서 연한 하늘색
```
```
# 무작위하게 섞여 있는게 불순도가 높은거
# 완벽하게 분리 되어 있는 것이 불순도가 낮은거
```

## Decision Tree를 나누는 기준

---

__Gini impurity(불순도)__

$$
Gini = 1 - \sum_i p_i^2
$$

*   $p_i$: $i$번째 클래스가 될 확률
*   이진(binary class) 분류
    *   gini = 1 - ((양성 클래스 확률)^2 + (음성 클래스 확률)^2)
    *   양성과 음성의 비율이 1:1인 경우, gini = 1 - (0.5^2 + 0.5^2) = 0.5. 불순도 최대.
    *   양성 또는 음성으로 완벽하게 분류된 경우, gini = 1 - 1 = 0. 불순도 최소.
*   Decision tree는 부모 노드와 자식 노드의 gini 불순도 차이가 가능한 커지도록 가지를 성장시킴.(트리를 분할할 때, 불순도가 가장 낮아지는 방향으로 데이터를 나눔)

---

__Entropy(엔트로피)__

$$
Entropy = -\sum_i p_i \log_k (p_i)
$$

*   $k$: 클래스의 개수(이진 분류인 경우, k=2).
*   $p_i$: $i$번째 클래스가 될 확률.
*   이진 분류
    *   양성과 음성의 비율이 1:1인 경우, Entropy = 1. 엔트로피 최대.
    *   양성 또는 음성으로 완벽히 분류된 경우, Entropy = 0. 엔트로피 최소.
*   Decision tree 객체를 생성할 때 criterion='entropy'라고 설정하면, 부모 노드와 자식 노드에서의 엔트로피 차이가 가능하면 커지도록 가지를 생성함.


## Decision Tree 특징

*   장점:
    *   특성들을 스케일링할 필요가 없다.
    *   결과를 이해하기 쉽다.
*   단점:
    *   과적합(overfitting)되기가 쉽다.
    *   여러가지 규제들을 적용해서 과적합 문제를 해결해야 함.
*   규제 하이퍼 파라미터(hyperparameter) - 생성자의 파라미터들
    *   `max_depth`: decision tree의 최대 깊이.
    *   `max_leaf_node`: leaf node의 최댓값.
    *   `max_features`: 각 노드에서 분할에 사용할 특성의 최대 개수.
    *   `min_samples_split`: 노드가 분할되기 위해서 가져야 할 최소 샘플 개수.
    *   `min_samples_leaf`: leaf 노드가 가져야 할 최소 샘플 개수.
    *   `max_`로 시작하는 파라미터의 값을 증가시키면, 트리의 크기가 커짐.
        *   규제가 작아짐.
        *   overfitting이 커짐.
    *   `max_`로 시작하는 파라미터 값을 감소시키면, 트리의 크기가 작아지기 때문에 overfitting이 작아짐.
    *   `min_`으로 시작하는 파라미터 값을 증가시키면, 트리의 크기가 작아짐.

 
## max_depth 파라미터 변화에 따른 정확도

```python
# 최대 깊이가 2인 decision tree
tree_clf = DecisionTreeClassifier(max_depth = 2, random_state = 42)
```
```python
tree_clf.fit(X_train, y_train)
```
```python
plt.figure(figsize = (12, 12))
plot_tree(tree_clf, feature_names=feature_names, class_names=target_names, filled = True)
plt.show()
```
<img width="870" height="796" alt="image" src="https://github.com/user-attachments/assets/be6e7355-dfe4-4aa4-a84c-14192f4bc7f5" />

```python
print('train acc.:', tree_clf.score(X_train, y_train))
print('test acc.:', tree_clf.score(X_test, y_test))
```
<img width="223" height="37" alt="image" src="https://github.com/user-attachments/assets/42aea8a0-151c-4a7d-aec3-3c94c095979e" />

```python
# 최대 깊이가 5인 decision tree
tree_clf = DecisionTreeClassifier(max_depth = 5, random_state = 42)
tree_clf.fit(X_train, y_train)
```
<img width="385" height="62" alt="image" src="https://github.com/user-attachments/assets/64524c3d-6167-4eb8-b8d0-ed19eab838b1" />

```python
plt.figure(figsize=(20, 25))
plot_tree(tree_clf, feature_names=feature_names, class_names=target_names, filled = True)
plt.show()
```
<img width="780" height="872" alt="image" src="https://github.com/user-attachments/assets/2e90040e-0b25-417f-9718-385b810c6b12" />

```python
print('train acc.:', tree_clf.score(X_train, y_train))
print('test acc.:', tree_clf.score(X_test, y_test))
```
<img width="224" height="29" alt="image" src="https://github.com/user-attachments/assets/e7bf9acc-8fe9-4832-9b26-66723a3f59b9" />

```python
depths = np.arange(2, 16)    # max_depth: 2~15
train_scores = []   # 훈련 셋에서의 정확도를 저장할 리스트
test_scores = []    # 테스트 셋에서의 정확도를 저장할 리스트

for d in depths:
    tree_clf = DecisionTreeClassifier(max_depth = d, random_state = 42)    # decision tree 생성
    tree_clf.fit(X_train, y_train)  # 훈련
    train_acc = tree_clf.score(X_train, y_train)    # 훈련 셋 정확도
    train_scores.append(train_acc)
    test_acc = tree_clf.score(X_test, y_test)    # 테스트 셋 정확도
    test_scores.append(test_acc)

print(train_scores)
print(test_scores)
```
```
<결과>
[0.8062343659803733, 0.8458726188185491, 0.8595343467385029, 0.8718491437367712, 0.8778141235328074, 0.8880123147969983, 0.9003271117952665, 0.9124494900904367, 0.9255339619010968, 0.941312295555128, 0.9538195112564941, 0.9680584952857417, 0.9757552434096595, 0.9834519915335771]
[0.8107692307692308, 0.8407692307692308, 0.8523076923076923, 0.8576923076923076, 0.8484615384615385, 0.8515384615384616, 0.8584615384615385, 0.8615384615384616, 0.8615384615384616, 0.8584615384615385, 0.8646153846153846, 0.8623076923076923, 0.8661538461538462, 0.8676923076923077]
```
```python
plt.plot(depths, train_scores, 'bo-', label = 'train')
plt.plot(depths, test_scores, 'ro:', label = 'test)
plt.legend()
plt.grid()
plt.xlabel('Tree Depth')
plt.ylabel('Accuracy')
plt.show()
```
<img width="542" height="392" alt="image" src="https://github.com/user-attachments/assets/6cd7733c-0b0b-4a91-b452-14e7caaa58b2" />


나무가 커질수록 둘다 좋아지다가 5넘어가면 test셋은 안좋아짐


# 하이퍼 파라미터 튜닝

*   전체 데이터를 훈련(train)-검증(validation)-테스트(test) 셋으로 나눔.
*   하이퍼 파라미터를 변경하면서 훈련 셋으로 ML 모델을 훈련시킴.
*   하이퍼 파라미터 설정에 따른 성능 테스트는 검증 셋으로 수행.
    *   검증 셋에서 점수가 좋은 (그리고 과대적합이 적은) 하이퍼 파라미터를 선택.
*   튜닝이 끝난 모델의 일반화 성능을 예측하기 위해서 테스트 셋을 사용.

```python
# 전체 데이터셋을 훈련/테스트 셋으로 나누기
X_tr_full, X_test, y_tr_full, y_test = train_test_split(X, y,
                                                        test_size = 0.2,
                                                        random_state = 42,
                                                        stratify = y)
```
```python
# X_tr_full을 훈련/검증 셋으로 나누기.
X_tr, X_val, y_tr, y_val = train_test_split(X_tr_full, y_tr_full,
                                            test_size=0.2,
                                            random_state = 42,
                                            stratify = y_tr_full)
```
```python
depths = np.arange(2, 16)
train_scores = []
val_scores = []
for d in depths:
    tree_clf = DecisionTreeClassifier(max_depth=d, random_state = 42)
    tree_clf.fit(X_tr, y_tr)
    train_scores.append(tree_clf.score(X_tr, y_tr))
    val_scores.append(tree_clf.score(X_val, y_val))
```
```python
plt.plot(depths, train_scores, 'bo-', label = 'train')
plt.plot(depths, val_scores, 'ro:', label = 'test')
plt.legend()
plt.grid()
plt.show()
```
<img width="516" height="371" alt="image" src="https://github.com/user-attachments/assets/c61ae08c-20f5-4150-a1df-c40fe6fd41ca" />

```python
train_scores[4], val_scores[4]  # max_depth = 6일 때 훈련 셋 정확도, 검증 셋 정확도
```
<img width="296" height="28" alt="image" src="https://github.com/user-attachments/assets/22bded85-6409-402d-b27e-583afee91dd0" />

## min_samples_split 하이퍼 파라미터 튜닝

```python
min_samples = [0.01, 0.05, 0.1, 0.15, 0.2]    
train_scores = []
val_scores = []
for s in min_samples:
    # min_samples_split = 0.01: node를 나누기 위한 기준 - 훈련 셋의 1%
    tree_clf = DecisionTreeClassifier(min_samples_split=s, random_state = 42)    # min_samples_split은 노드를 분할하기 위한 최소 샘플 수. 값이 작을수록 트리가 더 깊어지고 복잡해질 수 있음.
    tree_clf.fit(X_tr, y_tr)
    train_scores.append(tree_clf.score(X_tr, y_tr))
    val_scores.append(tree_clf.score(X_val, y_val))
```
```python
plt.plot(min_samples, train_scores, 'bo-', label = 'train')
plt.plot(min_samples, val_scores, 'ro:', label = 'validation')
plt.legend()
plt.grid()
plt.show()
```
<img width="516" height="379" alt="image" src="https://github.com/user-attachments/assets/bd9b2045-3d3a-45b4-9e30-5eab4e8e15b5" />

# 교차 검증(Cross Validation)

```python
from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict
```
```python
X_tr_full.shape
```
<img width="74" height="27" alt="image" src="https://github.com/user-attachments/assets/8cf975f1-3e9e-4f7e-bbd4-541a14a4bf50" />

```python
y_tr_full.shape
```
<img width="67" height="29" alt="image" src="https://github.com/user-attachments/assets/4f967db4-7e34-479c-9c7c-6f933a633ffc" />

```python
# 교차 검증에서 사용할 예측기(ML 모델) 생성
clf = DecisionTreeClassifier(max_depth=6, random_state=42)

# 5-fold 교차 검증(cross validation): 전체 훈련 셋을 5개로 나눠서 그 중 한 개를 검증으로 사용.
# 파라미터 cv: n-fold 교차 검증
# 파라미터 n_jobs: 동시에(병렬로) 수행할 작업 개수. -1이면 CPU의 모든 코어를 사용.(성능 좋게 하려면 -1 사용)
# 파라미터 return_train_score: 훈련 점수를 리턴할 것인 지.
cv = cross_validate(estimator=clf, X=X_tr_full, y=y_tr_full,
                    cv=5, n_jobs=-1, return_train_score=True)
```
```python
cv  # cross_validatate() 함수의 리턴 값은 dict 객체
```
<img width="605" height="76" alt="image" src="https://github.com/user-attachments/assets/3a66e23f-5d84-4eda-88ff-ffa438463046" />

```python
np.mean(cv['train_score'])  # 각 교차검증에서 훈련 셋 정확도들의 평균
# 교차검증 훈련 셋 점수 평균은 일반적인 훈련 셋 점수와 비슷하거나 약간 좋음.
```
<img width="226" height="24" alt="image" src="https://github.com/user-attachments/assets/5fee46af-8856-4e64-b3bb-c388a3dcdbf5" />

```python
np.mean(cv['test_score'])   # 각 교차검증에서 검증 셋 정확도들의 평균
```
<img width="223" height="30" alt="image" src="https://github.com/user-attachments/assets/581a1eeb-ddae-499b-a50f-2c6abdff44f9" />

```python
# n-fold 교차검증에서 각각의 검증 셋에서의 점수(정확도)들
cross_val_score(estimator = clf, X = X_tr_full, y = y_tr_full, cv = 5, n_jobs = -1)
# 점수만 보고 싶을 때 cross_val_scor()
```
<img width="489" height="30" alt="image" src="https://github.com/user-attachments/assets/65e21387-3328-43d8-b973-8a3f561c72b3" />

## 교차 검증을 사용한 max_depth 하이퍼 파라미터 튜닝

```python
train_scores = []
val_scores = []
max_depths = np.arange(2, 16)
for d in max_depths:
    tree = DecisionTreeClassifier(max_depth = d, random_state=42, )
    cv = cross_validate(estimator = tree, X = X_tr_full, y = y_tr_full,
                        cv = 5, n_jobs = -1, return_train_score=True)
    train_acc = np.mean(cv['train_score'])
    train_scores.append(train_acc)
    test_acc = np.mean(cv['test_score'])
    val_scores.append(test_acc)
```
```python
plt.plot(max_depths, train_scores, 'bo-', label = 'train')
plt.plot(max_depths, val_scores, 'r^:', label = 'validation')
plt.legend()
plt.grid()
plt.show()
```
<img width="526" height="382" alt="image" src="https://github.com/user-attachments/assets/f227173e-3ce4-4949-a420-99ff004e271f" />

x축은 tree의 depths

```python
np.max(val_scores)
```
<img width="223" height="28" alt="image" src="https://github.com/user-attachments/assets/41a9da2a-a254-4cc4-844c-ce81f6841f96" />

```python
np.argmax(val_scores)   # 4번째 인덱스에서 최댓값
```
<img width="95" height="28" alt="image" src="https://github.com/user-attachments/assets/e5fefa5c-2551-4a69-bb9e-c46f6d2fb04c" />

```python
max_depths[np.argmax(val_scores)]
```
<img width="90" height="26" alt="image" src="https://github.com/user-attachments/assets/d0de5683-ff88-4c8d-be6f-e2f9b147196c" />

# Grid Search Cross Validation

```python
from sklearn.model_selection import GridSearchCV
```
```python
# 파라미터 튜닝을 하려는 ML 모델(예측기) 생성
tree = DecisionTreeClassifier(random_state=42)
```
```python
# 튜닝할 하이퍼 파라미터들의 조합을 dict로 만듦. 키는 ML 클래스 생성자의 파라미터 이름을 사용.
params = {'max_depth': np.arange(2, 21),
          'min_samples_split': np.arange(2, 100, 2)}
```
```python
# GridSearchCV 객체 생성
grid_cv = GridSearchCV(estimator = tree, param_grid = params, n_jobs = -1)
```
```python
# 훈련 -> 5-fold 교차 검증 수행하면서 최적의 파라미터 조합을 찾음.
grid_cv.fit(X_tr_full, y_tr_full)
```
<img width="354" height="142" alt="image" src="https://github.com/user-attachments/assets/314595cb-2324-48d6-8e9d-1d17f4020e77" />

```python
grid_cv.best_params_    # 교차 검증의 test_score를 최대로 만들어 주는 파라미터 조합
```
<img width="446" height="31" alt="image" src="https://github.com/user-attachments/assets/ddd5ef3a-0813-4b6e-9712-54a171a0ee0f" />

```python
grid_cv.best_score_     # 교차 검증에서 test_score 최댓값.
```
<img width="226" height="26" alt="image" src="https://github.com/user-attachments/assets/132128d5-b6f9-464c-b590-da6897838b69" />

```python
grid_cv.best_estimator_
```
<img width="567" height="81" alt="image" src="https://github.com/user-attachments/assets/c8e99f31-bd8d-4434-94d0-676069df5a46" />

```python
best_tree = grid_cv.best_estimator_
best_tree
```
<img width="554" height="84" alt="image" src="https://github.com/user-attachments/assets/cc6e70ef-dda7-4e85-b013-a8afe8aa432f" />

```python
best_tree.score(X_test, y_test)
```
<img width="144" height="24" alt="image" src="https://github.com/user-attachments/assets/4f74e49e-8f10-4b90-816e-c85286bda4de" />
