# Decision Tree

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report
```

## 데이터 셋

```python
# wine 데이터셋
wine_csv = 'https://bit.ly/wine_csv_data'
```
```python
wine_df = pd.read_csv(wind_csv)
```
```python
wine_df.head()
```
<img width="243" height="176" alt="image" src="https://github.com/user-attachments/assets/24d9f8ac-c77d-418d-bb2d-3fb2e53f3a99" />

```python
wine_df.info()
```
<img width="282" height="181" alt="image" src="https://github.com/user-attachments/assets/91264171-4be5-4208-9902-23cff590147e" />

```python
wine_df.describe()
```
<img width="401" height="256" alt="image" src="https://github.com/user-attachments/assets/53d0dcfe-dc43-4c10-bedc-d32bc20f686e" />

```python
class_counts = wine_df['class'].value_counts()
```
```python
class_counts   # 0, 1만 있는 카테고리(0 - red wine, 1 - white wine)
```
<img width="126" height="157" alt="image" src="https://github.com/user-attachments/assets/77371d27-dcf7-44d4-b125-0f908e9f5d2e" />

```python
wind_df.columns[0:3]
```
<img width="361" height="33" alt="image" src="https://github.com/user-attachments/assets/6e01316e-daa8-4ecb-aea2-74270a3ffec6" />

## alcohol, sugar, pH 변수들의 히스토그램

```python
fig, axes = plt.subplots(ncols = 3, figsize = (16, 4))

for i, col in enumerate(wine_df.columns[:3]):
    sns.hisplot(data = wine_df, x = col, ax = axes[i])
plt.show()
```
<img width="844" height="240" alt="image" src="https://github.com/user-attachments/assets/b1e5917b-30cc-4572-8a7d-a072764388a9" />

## class 별로 색깔을 다르게 시각화한 pairplot

```python
sns.pairplot(data = wine_df, hue = 'class')
plt.show()
```
<img width="771" height="669" alt="image" src="https://github.com/user-attachments/assets/3e0531b0-65d4-49ce-a233-796a47bdf84f" />

## 훈련/테스트 나누기

```python
X = wind_df[wine_df.columns[:3]].values    # 특성 배열(alcohol, sugar, pH)
y = wine_df['class'].values    # 타겟 배열(class)
```
```python
X[:5, :]
```
<img width="204" height="86" alt="image" src="https://github.com/user-attachments/assets/33b0799c-28e9-4932-82fb-4090a763a985" />

```python
y[:5]
```
<img width="205" height="28" alt="image" src="https://github.com/user-attachments/assets/2ab8f472-7711-4fac-8894-391536316336" />

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)
```
```python
X_train.shape
```
<img width="80" height="32" alt="image" src="https://github.com/user-attachments/assets/6b28f291-6df9-4c8a-ad65-5596fba14e59" />

```python
X_test.shape
```
<img width="79" height="32" alt="image" src="https://github.com/user-attachments/assets/2efdb729-d55b-40a1-8bab-9b166be3eb03" />

```python
y_train.shape
```
<img width="66" height="30" alt="image" src="https://github.com/user-attachments/assets/e8f69f82-a485-430a-83f5-97d644e08f5f" />

```python
y_test.shape
```
<img width="66" height="31" alt="image" src="https://github.com/user-attachments/assets/41840277-9c94-462c-a749-4d80bc04bc5d" />

```python
feature_names = wine_df.columns[:3]    # 특성 이름
target_names = ['red', 'white']    # 클래스 이름(레이블)
print(feature_names)
print(target_names)
```
<img width="354" height="43" alt="image" src="https://github.com/user-attachments/assets/978cdb25-3551-4dc0-8ff7-12609506af6f" />

# Decision Tree

```python
tree_clf = DecisionTreeClassifier(random_state = 42)    # 모델 생성
```
```python
tree_clf.fit(X_train, y_train)    # 모델 훈련
```
<img width="308" height="71" alt="image" src="https://github.com/user-attachments/assets/5c8d458f-c4e7-4587-9e39-6dc82741d678" />

```python
train_pred = tree_clf.predict(X_train)    # 훈련 셋 예측값
```
```python
cm_train = confusion_matrix(y_train, train_pred)
cm_train
```
<img width="158" height="46" alt="image" src="https://github.com/user-attachments/assets/71115176-6239-4f50-8287-2f176a8b732e" />

```python
sns.heatmap(data = cm_train, cmap = 'Blues', annot = True, fmt = 'd',
            xticklabels = target_names, yticklabels = target_names)
plt.show()
```
<img width="477" height="379" alt="image" src="https://github.com/user-attachments/assets/dc2ab864-dbe6-4198-978d-9aa3bc2eccce" />

```python
print(classification_report(y_train, train_pred))
```
<img width="396" height="146" alt="image" src="https://github.com/user-attachments/assets/214c3ae4-6559-4880-9340-8f671bc2fde6" />

```python
test_pred = tree_clf.predict(X_test)    # 테스트 셋 예측값
```
```python
cm_test = confusion_matrix(y_test, test_pred)
cm_test
```
<img width="149" height="44" alt="image" src="https://github.com/user-attachments/assets/19bffbe3-53e9-4395-8153-67214f64a8b7" />

```python
sns.heatmap(data = cm_test, cmap = 'Blues', annot = True, fmt = 'd',
            xticklabels = target_names, yticklabels = target_names)
plt.show()
```
<img width="479" height="371" alt="image" src="https://github.com/user-attachments/assets/a8b4e930-1b9a-4114-a94b-1ce9d74a004a" />

```python
print(classification_report(y_test, test_pred))
```
<img width="400" height="135" alt="image" src="https://github.com/user-attachments/assets/72a4315c-7a35-4958-a933-2bcf2616ff12" />


현재까지 훈련된 Decision Tree는 과대적합이 매우 크다.


```python
plot_tree(tree_clf)
plt.show()
```
<img width="484" height="350" alt="image" src="https://github.com/user-attachments/assets/df063a68-df55-4ded-b6c3-aba05ecb54e6" />

```python
plt.figure(figsize = (16, 12))
plot_tree(tree_clf, max_depth = 2, feature_names = feature_names, class_names = target_names, filled = True)
# feature_names 특성 이름 무엇으로 할거냐. filled : 노드들의 색을 채울거냐
plt.show()
```
<img width="844" height="622" alt="image" src="https://github.com/user-attachments/assets/4a23bc9f-a8b5-41e3-8d05-27d559f4985b" />

```
# 레드가 많으면 많을 수록 오렌지 색이 진해짐, 화이트가 많을 수록 파란색으로 됨
# 당도가 4.325보다 작거나 같으면 왼쪽. 
# samples은 총 수, value = [1199, 1747] (레드 수, 화이트 수) 두개의 수 중에 많은걸로 class 정함. 
# 레드와 화이트 수가 차이 많이 안나서 연한 하늘색
```
```
# 무작위하게 섞여 있는게 불순도가 높은거
# 완벽하게 분리 되어 있는 것이 불순도가 낮은거
```

## Decision Tree를 나누는 기준

---

__Gini impurity(불순도)__

$$
Gini = 1 - \sum_i p_i^2
$$

*   $p_i$: $i$번째 클래스가 될 확률
*   이진(binary class) 분류
    *   gini = 1 - ((양성 클래스 확률)^2 + (음성 클래스 확률)^2)
    *   양성과 음성의 비율이 1:1인 경우, gini = 1 - (0.5^2 + 0.5^2) = 0.5. 불순도 최대.
    *   양성 또는 음성으로 완벽하게 분류된 경우, gini = 1 - 1 = 0. 불순도 최소.
*   Decision tree는 부모 노드와 자식 노드의 gini 불순도 차이가 가능한 커지도록 가지를 성장시킴.(트리를 분할할 때, 불순도가 가장 낮아지는 방향으로 데이터를 나눔)

---

__Entropy(엔트로피)__

$$
Entropy = -\sum_i p_i \log_k (p_i)
$$

*   $k$: 클래스의 개수(이진 분류인 경우, k=2).
*   $p_i$: $i$번째 클래스가 될 확률.
*   이진 분류
    *   양성과 음성의 비율이 1:1인 경우, Entropy = 1. 엔트로피 최대.
    *   양성 또는 음성으로 완벽히 분류된 경우, Entropy = 0. 엔트로피 최소.
*   Decision tree 객체를 생성할 때 criterion='entropy'라고 설정하면, 부모 노드와 자식 노드에서의 엔트로피 차이가 가능하면 커지도록 가지를 생성함.


## Decision Tree 특징

*   장점:
    *   특성들을 스케일링할 필요가 없다.
    *   결과를 이해하기 쉽다.
*   단점:
    *   과적합(overfitting)되기가 쉽다.
    *   여러가지 규제들을 적용해서 과적합 문제를 해결해야 함.
*   규제 하이퍼 파라미터(hyperparameter) - 생성자의 파라미터들
    *   `max_depth`: decision tree의 최대 깊이.
    *   `max_leaf_node`: leaf node의 최댓값.
    *   `max_features`: 각 노드에서 분할에 사용할 특성의 최대 개수.
    *   `min_samples_split`: 노드가 분할되기 위해서 가져야 할 최소 샘플 개수.
    *   `min_samples_leaf`: leaf 노드가 가져야 할 최소 샘플 개수.
    *   `max_`로 시작하는 파라미터의 값을 증가시키면, 트리의 크기가 커짐.
        *   규제가 작아짐.
        *   overfitting이 커짐.
    *   `max_`로 시작하는 파라미터 값을 감소시키면, 트리의 크기가 작아지기 때문에 overfitting이 작아짐.
    *   `min_`으로 시작하는 파라미터 값을 증가시키면, 트리의 크기가 작아짐.

 
## max_depth 파라미터 변화에 따른 정확도

```python
# 최대 깊이가 2인 decision tree
tree_clf = DecisionTreeClassifier(max_depth = 2, random_state = 42)
```
```python
tree_clf.fit(X_train, y_train)
```
```python
plt.figure(figsize = (12, 12))
plot_tree(tree_clf, feature_names=feature_names, class_names=target_names, filled = True)
plt.show()
```
<img width="870" height="796" alt="image" src="https://github.com/user-attachments/assets/be6e7355-dfe4-4aa4-a84c-14192f4bc7f5" />

```python
print('train acc.:', tree_clf.score(X_train, y_train))
print('test acc.:', tree_clf.score(X_test, y_test))
```
<img width="223" height="37" alt="image" src="https://github.com/user-attachments/assets/42aea8a0-151c-4a7d-aec3-3c94c095979e" />

```python
# 최대 깊이가 5인 decision tree
tree_clf = DecisionTreeClassifier(max_depth = 5, random_state = 42)
tree_clf.fit(X_train, y_train)
```
<img width="385" height="62" alt="image" src="https://github.com/user-attachments/assets/64524c3d-6167-4eb8-b8d0-ed19eab838b1" />

```python
plt.figure(figsize=(20, 25))
plot_tree(tree_clf, feature_names=feature_names, class_names=target_names, filled = True)
plt.show()
```
<img width="780" height="872" alt="image" src="https://github.com/user-attachments/assets/2e90040e-0b25-417f-9718-385b810c6b12" />

```python
print('train acc.:', tree_clf.score(X_train, y_train))
print('test acc.:', tree_clf.score(X_test, y_test))
```
<img width="224" height="29" alt="image" src="https://github.com/user-attachments/assets/e7bf9acc-8fe9-4832-9b26-66723a3f59b9" />

```python
depths = np.arange(2, 16)    # max_depth: 2~15
train_scores = []   # 훈련 셋에서의 정확도를 저장할 리스트
test_scores = []    # 테스트 셋에서의 정확도를 저장할 리스트

for d in depths:
    tree_clf = DecisionTreeClassifier(max_depth = d, random_state = 42)    # decision tree 생성
    tree_clf.fit(X_train, y_train)  # 훈련
    train_acc = tree_clf.score(X_train, y_train)    # 훈련 셋 정확도
    train_scores.append(train_acc)
    test_acc = tree_clf.score(X_test, y_test)    # 테스트 셋 정확도
    test_scores.append(test_acc)

print(train_scores)
print(test_scores)
```
```
<결과>
[0.8062343659803733, 0.8458726188185491, 0.8595343467385029, 0.8718491437367712, 0.8778141235328074, 0.8880123147969983, 0.9003271117952665, 0.9124494900904367, 0.9255339619010968, 0.941312295555128, 0.9538195112564941, 0.9680584952857417, 0.9757552434096595, 0.9834519915335771]
[0.8107692307692308, 0.8407692307692308, 0.8523076923076923, 0.8576923076923076, 0.8484615384615385, 0.8515384615384616, 0.8584615384615385, 0.8615384615384616, 0.8615384615384616, 0.8584615384615385, 0.8646153846153846, 0.8623076923076923, 0.8661538461538462, 0.8676923076923077]
```
```python
plt.plot(depths, train_scores, 'bo-', label = 'train')
plt.plot(depths, test_scores, 'ro:', label = 'test)
plt.legend()
plt.grid()
plt.xlabel('Tree Depth')
plt.ylabel('Accuracy')
plt.show()
```
<img width="542" height="392" alt="image" src="https://github.com/user-attachments/assets/6cd7733c-0b0b-4a91-b452-14e7caaa58b2" />


나무가 커질수록 둘다 좋아지다가 5넘어가면 test셋은 안좋아짐


# 하이퍼 파라미터 튜닝

*   전체 데이터를 훈련(train)-검증(validation)-테스트(test) 셋으로 나눔.
*   하이퍼 파라미터를 변경하면서 훈련 셋으로 ML 모델을 훈련시킴.
*   하이퍼 파라미터 설정에 따른 성능 테스트는 검증 셋으로 수행.
    *   검증 셋에서 점수가 좋은 (그리고 과대적합이 적은) 하이퍼 파라미터를 선택.
*   튜닝이 끝난 모델의 일반화 성능을 예측하기 위해서 테스트 셋을 사용.
