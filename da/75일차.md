# CNN  연습

*   Keras 패키지의 손글씨 MNIST 예제 데이터셋을 다운로드하세요.
*   예제 데이터셋의 일부(예: 100개)를 시각화해 보세요.
*   훈련 셋과 테스트 셋의 데이터는 0. ~ 1. 스케일로 스케일링하세요.
*   전체 훈련 셋은 다시 훈련 셋과 검증 셋으로 나누세요.
*   아래의 CNN 모델 구조처럼 합성곱 신경망을 설계하세요.
*   모든 합성곱 층의 커널 사이즈는 3으로 설정하세요.
*   모든 합성곱 층(Conv2D)와 출력층을 제외한 밀집층(Dense)에서
    *   활성화 함수는 ReLU로 설정하세요.
    *   커널 초기화 함수(kernel initializer)를 'he_normal'로 설정하세요.
*   첫번째 Dropout 층의 rate는 0.25로 설정하세요.
*   두번째 Dropout 층의 rate는 0.5로 설정하세요.
*   모델의 옵티마이저(optimizer)는 Nadam을 사용하세요.
*   ModelCheckpoint, EarlyStopping 콜백들과 검증 셋을 함께 사용해서 모델을 훈련하세요.
    *   최적의 모델은 cnn_ex_best.keras 파일로 저장하세요.
*   모델의 성능을 평가하세요.

---

__CNN example__

<img src="https://raw.githubusercontent.com/JakeOh/202505_BD50/refs/heads/main/lab_da/cnn_example_64dpi.png" alt="CNN example" />


------- 내가 푼거
# Imports

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
import tensorflow as tf
import keras
```

# MNIST 데이터 셋

```python
mnist = datasets.fetch_openml('mnist_784')
```
```python
X = mnist.data.values.copy()
y = mnist.target.values.copy()
```

# 예제 데이터셋의 일부(예: 100개)를 시각화

```python
fig, axes = plt.subplots(nrows = 10, ncols = 10)
for i in range(10):
    for j in range(10):
        img = X[i * 10 + j].reshape((28, 28))
        axes[i][j].imshow(img, cmap = 'binary')
        axes[i][j].axis('off')

plt.show()
```
<img width="505" height="389" alt="image" src="https://github.com/user-attachments/assets/5c9ed4a7-b7f1-416a-bee7-19bb3e7ef808" />

# 훈련 셋/테스트 셋 나누기

```python
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = 10_000, stratify = y, random_state=42)
```
```python
print(X_tr.shape)
print(y_tr.shape)
print(X_te.shape)
print(y_te.shape)
```
<img width="96" height="74" alt="image" src="https://github.com/user-attachments/assets/7b7e1b89-8d5d-4cbe-b81e-19b7123f47d7" />

# 훈련 셋과 테스트 셋의 데이터는 0. ~ 1. 스케일로 스케일링

```python
X_tr_scaled = X_tr.reshape((-1, 28, 28, 1)) / 255.0
X_te_scaled = X_te.reshape((-1, 28, 28, 1)) / 255.0
```

# 전체 훈련 셋은 다시 훈련 셋과 검증 셋으로 나누기

```python
X_train, X_val, y_train, y_val = train_test_split(X_tr_scaled, y_tr, test_size = 0.2, stratify = y_tr)
```
```python
y_train = y_train.astype(np.int32)
y_val = y_val.astype(np.int32)
y_te = y_te.astype(np.int32)
```
```python
print(X_train.shape)
print(X_val.shape)
```
<img width="143" height="41" alt="image" src="https://github.com/user-attachments/assets/e4b5bb22-f019-4423-87cc-382a2f631f96" />

```python
model = keras.Sequential()
```
```python
inputs = keras.Input(shape = (28, 28, 1))
model.add(inputs)
```
```python
conv_1 = keras.layers.Conv2D(filters = 32, kernel_size = 3,
                             padding = 'same', activation = 'relu', kernel_initializer = 'he_normal')
model.add(conv_1)
```
```python
conv_2 = keras.layers.Conv2D(filters = 64, kernel_size = 3,
                             padding = 'same', activation = 'relu', kernel_initializer = 'he_normal')
model.add(conv_2)
```
```python
max_pool_1 = keras.layers.MaxPool2D()
model.add(max_pool_1)
```
```python
model.add(keras.layers.Flatten())
```
```python
model.add(keras.layers.Dropout(rate = 0.25))
```
```python
dense_1 = keras.layers.Dense(units = 128, activation='relu', kernel_initializer = 'he_normal')
model.add(dense_1)
```
```python
model.add(keras.layers.Dropout(rate = 0.5))
```
```python
dense_2 = keras.layers.Dense(units = 10, activation='softmax')
model.add(dense_2)
```
```python
model.summary()
```
<img width="559" height="353" alt="image" src="https://github.com/user-attachments/assets/ea72ff1d-34ae-4514-aeae-46f0791a1859" />

```python
keras.utils.plot_model(model, dpi = 64, show_shapes = True)
```
<img width="399" height="828" alt="image" src="https://github.com/user-attachments/assets/3ae8a3db-6dd9-465a-9696-c7243369de0c" />

```python
model.compile(optimizer = keras.optimizers.Nadam(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
dir_path = '/content/drive/MyDrive/Colab Notebooks/lab_da/'
model_file = dir_path + 'cnn_ex_best.keras'

checkpoint = keras.callbacks.ModelCheckpoint(model_file, save_best_only=True)
early_stop = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
```
```python
result = model.fit(x = X_train, y = y_train, epochs = 100,
                   callbacks = [checkpoint, early_stop],
                   validation_data = [X_val, y_val])
```
<img width="1280" height="261" alt="image" src="https://github.com/user-attachments/assets/562d5e78-717f-4ce6-8cc8-c4ce327bcdea" />

# 모델 평가

```python
model.evaluate(x = X_train, y = y_train)
```
<img width="849" height="46" alt="image" src="https://github.com/user-attachments/assets/e790c196-f310-40e5-821d-a342115b0be6" />

```python
model.evaluate(x = X_val, y = y_val)
```
<img width="822" height="37" alt="image" src="https://github.com/user-attachments/assets/98994b8e-3c20-40c6-a702-4fa0690c9fa5" />



------- 풀이
# Import

```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

import tensorflow as tf
import keras
```
```python
tf.config.list_physical_devices()
```
<img width="473" height="27" alt="image" src="https://github.com/user-attachments/assets/1b97bec9-5381-4796-97f2-39bf67ee2592" />

# MNIST Datasets

```python
(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()
```
<img width="668" height="44" alt="image" src="https://github.com/user-attachments/assets/27581b8e-133c-4eb3-a54a-18b6355f95dc" />

```python
print(x_train_full.shape)
print(y_train_full.shape)
print(x_test.shape)
print(y_test.shape)
```
```python
np.unique(y_train_full, return_counts = True)  # 고유한 값들을 찾고, 각 고유한 값이 몇 번 나타나는 지
```
<img width="498" height="43" alt="image" src="https://github.com/user-attachments/assets/bb933130-e689-4109-8210-b33d1e9620b1" />

```python
np.unique(y_test, return_counts = True)
```
<img width="499" height="47" alt="image" src="https://github.com/user-attachments/assets/ac0c28fd-d523-40bc-825c-5105736a1aa6" />

```python
def plot_mnist_image(arr, ncols = 10):  # ncols = 10 한 줄에 이미지 10개
    # arr: (n_samples, height, width) 또는 (n_samples, height, width, n_channels) 모양의 배열.
    n_samples = len(arr)
    nrows = int(np.ceil(n_samples / ncols))  # 올림
    fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (ncols, nrows))  # figsize는 width, height 순서로 줘야함
    for i in range(nrows):
        for j in range(ncols):
            idx = i * ncols + j
            if nrows == 1 or ncols == 1:
                if idx < n_samples:
                    ax[idx].imshow(arr[idx], cmap = plt.cm.binary)
                ax[idx].axis('off')
            else:
                if idx < n_samples:
                    ax[i, j].imshow(arr[idx], cmap = plt.cm.binary)
                ax[i, j].axis('off')
    plt.show()
```
```python
plot_mnist_image(x_train_full[:50])
```
<img width="794" height="404" alt="image" src="https://github.com/user-attachments/assets/8a5934cb-ab40-4fca-ad91-2ec52aa8e402" />

```python
plot_mnist_image(x_test[:3], ncols = 1)
```
<img width="87" height="251" alt="image" src="https://github.com/user-attachments/assets/76b22598-6ede-4453-901e-0e20a41d31f4" />


# 데이터 셋 reshape & scaling

*   이미지 분류 합성곱 신경망에서는 이미지 배열의 모양이 (n_samples, height, width, n_channel)인 3차원 배열이라고 가정.
*   0. ~ 1. 실수 범위인 경우가 성능이 좋음.


```python
np.max(x_train_full[0]), np.min(x_train_full[0])
# 첫번째 이미지 픽셀의 최댓값, 최소값 -> 0 ~ 255 사이의 부호 없는 8비트 정수.
```
<img width="204" height="30" alt="image" src="https://github.com/user-attachments/assets/89190731-f111-4a9c-83bd-4a943255bd7f" />

```python
x_train_full_scaled = x_train_full.reshape((-1, 28, 28, 1)) / 255.
x_test_scaled = x_test.reshape((-1, 28, 28, 1)) / 255.
```
```python
x_train_full_scaled.shape
```
<img width="141" height="29" alt="image" src="https://github.com/user-attachments/assets/634c10c9-b2d7-475d-9e01-ce65dd2abb26" />

```python
np.max(x_train_full_scaled), np.min(x_train_full_scaled)
```
<img width="254" height="32" alt="image" src="https://github.com/user-attachments/assets/fc26db95-ab4c-48c9-bfc3-c82b8ec21a48" />

# 훈련/검증 셋 분리

```python
x_train, x_val, y_train, y_val = train_test_split(x_train_full_scaled, y_train_full,
                                                  test_size = 0.1,
                                                  stratify = y_train_full,
                                                  random_state = 42)
```
```python
x_train.shape
```
<img width="139" height="27" alt="image" src="https://github.com/user-attachments/assets/cab8f7d3-b5d1-41e2-9a3b-141e5fdf4685" />

# 합성곱 신경망 생성

```python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.Sequential(layers = [
    keras.Input(shape = (28, 28, 1)),  # 입력층
    keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same',  # filters의 갯수만큼 output이 나옴
                        activation = 'relu', kernel_initializer = 'he_normal'),  # kernel_initializer -> 커널 초기화해주는 함수. 첫번째 합성곱
    keras.layers.conv2D(filters = 64, kernel_size = 3, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),  # 두번째 합성곱
    keras.layers.MaxPool2D(),  # Max Pooling
    keras.layers.Flatten(),
    keras.layers.Dropout(rate = 0.25),
    keras.layers.Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal'),  # 밀집층
    keras.layers.Dropout(rate = 0.5),
    keras.layers.Dense(units = 10, activation = 'softmax')  # 마지막 dense들은 나가는 값들만 있으면 되기 때문에 kernel_initializer를 설정하지 않음

])
```
```python
model.summary()
```
<img width="555" height="355" alt="image" src="https://github.com/user-attachments/assets/a4204539-77a5-41f9-bc7f-715b0db2fd31" />

```
(3 * 3 + 1) * 32 = 320
(3 * 3 * 32 + 1) * 64 = 18,496 -> bias = 1, filters = 64
14 * 14 * 64 = 12544 -> 세로 * 가로 * 채널수
(12944 + 1) * 128 = 1,605,760 -> bias = 1, units = 128
(128 + 1) * 10 = 1,290

메모리 계산
4 *  1625866 / 1024 / 1024 = 6.2021... -> 4바이트 
```

# 모델 훈련

```python
# 모델 컴파일(최적화함수, 손실함수, 평가지표)
model.compile(optimizer = 'nadam',
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy)
```
```python
from datetime import datetime

dir_path = '/content/drive/MyDrive/Colab Notebooks/lab_da/'
file_name = 'cnn_ex_best'
time_suffix = datetime.now().strftime('_%Y%m%d%H%M%S')    # 현재 시간 정보
file_path = f'{dir_path}{file_name}{time_suffix}.keras'

print(file_path)
```
<img width="564" height="30" alt="image" src="https://github.com/user-attachments/assets/2fc6622e-f2c7-4f4c-8c1c-11fe65b448d6" />

```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = file_path, save_best_only = True)
early_stop = keras.callbacks.EarlyStopping(patience = 2)
```
```python
result = model.fit(x = x_train, y = y_train, epochs = 100,
                    callbacks = [checkpoint, early_stop],
                    validation_data = [x_val, y_val])
```
<img width="1276" height="138" alt="image" src="https://github.com/user-attachments/assets/3d6b0161-1d69-4a02-acfe-9b21decbeae1" />

```python
def plot_train_val_loss(history):
    # history: keras의 History 클래스 타입 객체. fit 메서드의 리턴 값.
    epochs = history.epoch
    history = history.history
    plt.plot(epochs, history['loss'], 'bo-', label = 'train loss')
    plt.plot(epochs, history['val_loss'], 'ro:', label = 'validation loss')
    plt.legend()
    plt.grid()
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.show()
```
```python
plot_train_val_loss(result)
```
<img width="576" height="432" alt="image" src="https://github.com/user-attachments/assets/771cd967-4c15-43f5-b397-462d89a9cd00" />

# 모델 평가

```python
model.evaluate(x = x_train, y = y_train)    # 훈련 셋 손실/정확도
```
<img width="831" height="43" alt="image" src="https://github.com/user-attachments/assets/62ef73ba-4b02-4ae6-901a-d3948ed547aa" />

```python
model.evaluate(x = x_val, y = y_val)    # 검증 셋 손실/정확도
```
<img width="818" height="45" alt="image" src="https://github.com/user-attachments/assets/c1add540-e6f1-4a68-9c81-80a91861747f" />

```python
model.evaluate(x = x_test_scaled, y = y_test)   # 테스트 셋 손실/정확도
```
<img width="820" height="44" alt="image" src="https://github.com/user-attachments/assets/e8559272-f4e3-470c-8e29-47fa9cbda7e3" />

--------------------------

# CNN 연습 2

*   Fashion MNIST 데이터 셋
*   CNN 구조
    *   Conv2D(filters = 64, kernel_size = 7)
    *   MaxPool2D
    *   Conv2D(filters = 128, kernel_size = 3)
    *   Conv2D(filters = 128, kernel_size = 3)
    *   MaxPool2D
    *   Conv2D(filters = 256, kernel_size = 3)
    *   Conv2D(filters = 256, kernel_size = 3)
    *   MaxPool2D
    *   Flatten
    *   Dense(units = 128)
    *   Dropout(rate = 0.5)
    *   Dense(units = 64)
    *   Dropout(rate = 0.5)
    *   Dense(units = 10) - 출력층(units 수 알아서 정하기)
    *   출력층을 제외한 층(Conv2D, Dense)들의 활성화 함수는 relu
    *   합성곱은 same 패딩 방식.
    *   커널 초기화 함수 'he_normal'
 
```python
(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
```
<img width="804" height="138" alt="image" src="https://github.com/user-attachments/assets/1be51b41-9b3d-4210-aacb-87438bf57661" />

```python
print(x_train_full.shape)
print(y_train_full.shape)
print(x_test.shape)
print(y_test.shape)
```
<img width="120" height="77" alt="image" src="https://github.com/user-attachments/assets/fdc8dab1-d4f5-4884-9d01-7417e7f2778a" />

```python
np.unique(y_train_full, return_counts = True)
```
<img width="498" height="42" alt="image" src="https://github.com/user-attachments/assets/a96259cd-41d2-4370-81e8-bd8cacf788e3" />

```python
np.unique(y_test, return_counts = True)
```
<img width="504" height="43" alt="image" src="https://github.com/user-attachments/assets/385025b4-7cf6-4743-937c-ab2e3c06bc66" />

```python
def plot_mnist_fashion_image(arr, ncols=10):
    n_samples = len(arr)
    nrows = int(np.ceil(n_samples / ncols))
    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols, nrows))
    for i in range(nrows):
        for j in range(ncols):
            idx = i * ncols + j
            if nrows == 1 or ncols == 1:
                if idx < n_samples:
                    ax[idx].imshow(arr[idx], cmap=plt.cm.binary)
                ax[idx].axis('off')
            else:
                if idx < n_samples:
                    ax[i, j].imshow(arr[idx], cmap=plt.cm.binary)
                ax[i, j].axis('off')
    plt.show()
```
```python
plot_mnist_fashion_image(x_train_full[:50])
```
<img width="794" height="404" alt="image" src="https://github.com/user-attachments/assets/2da44346-cb7a-4a0c-a23d-59b41a77b4fd" />

```python
x_train_full_scaled = x_train_full.reshape((-1, 28, 28, 1)) / 255.
x_test_scaled = x_test.reshape((-1, 28, 28, 1)) / 255.
```
```python
x_train, x_val, y_train, y_val = train_test_split(x_train_full_scaled, y_train_full,
                                                  test_size = 0.1,
                                                  stratify = y_train_full,
                                                  random_state = 42)
```
```python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.Sequential(layers = [
    keras.Input(shape = (28, 28, 1)),
    keras.layers.Conv2D(filters = 64, kernel_size = 7, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),
    keras.layers.MaxPool2D(),
    keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),
    keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),
    keras.layers.MaxPool2D(),
    keras.layers.Conv2D(filters = 256, kernel_size = 3, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),
    keras.layers.Conv2D(filters = 256, kernel_size = 3, padding = 'same',
                        activation = 'relu', kernel_initializer = 'he_normal'),
    keras.layers.MaxPool2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(units = 128, activation = 'relu', kernel_initializer= 'he_normal'),
    keras.layers.Dropout(rate = 0.5),
    keras.layers.Dense(units = 64, activation = 'relu', kernel_initializer= 'he_normal'),
    keras.layers.Dropout(rate = 0.5),
    keras.layers.Dense(units = 10, activation = 'softmax')                           
])
```
```python
model.summary()
```
<img width="570" height="535" alt="image" src="https://github.com/user-attachments/assets/baa48e62-8318-4039-93ed-c174013aeaee" />

```python
keras.utils.plot_model(model, show_shapes = Ture, dpi = 64)
```
<img width="412" height="1468" alt="image" src="https://github.com/user-attachments/assets/63298748-b0d4-4109-a79f-0fb4a56bcf54" />

```python
model.compile(optimizer = keras.optimizers.Nadam(),
              loss = keras.losses.sparse_categorical_crossentropy,
              metrics = [keras.metrics.sparse_categorical_accuracy])
```
```python
from datetime import datetime

dir_path = '/content/drive/MyDrive/Colab Notebooks/lab_da/'
file_name = 'cnn_ex_fashion_best'
time_suffix = datetime.now().strftime('_%Y%m%d%H%M%S')
file_path = f'{dir_path}{file_name}{time_suffix}.keras'

print(file_path)
```
<img width="627" height="27" alt="image" src="https://github.com/user-attachments/assets/653db450-7e87-46fd-97c3-7e78c575d3df" />

```python
checkpoint = keras.callbacks.ModelCheckpoint(filepath = file_path, save_best_only = True)
early_stop = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)
```
```python
result = model.fit(x = x_train, y = y_train, epochs = 100,
                   callbacks = [checkpoint, early_stop],
                   validation_data = [x_val, y_val])
```
<img width="1268" height="169" alt="image" src="https://github.com/user-attachments/assets/00b7558d-63bf-4ba3-893f-8dcc74187753" />

```python
plot_train_val_loss(result)
```
<img width="567" height="432" alt="image" src="https://github.com/user-attachments/assets/501b1432-45c7-4c43-8025-7577c03e8d04" />

```python
model.evaluate(x_train, y_train)
```
<img width="827" height="45" alt="image" src="https://github.com/user-attachments/assets/d11b3632-69b9-431f-83d4-bd1607ef3214" />

```python
model.evaluate(x_val, y_val)
```
<img width="820" height="45" alt="image" src="https://github.com/user-attachments/assets/d167f280-8985-46de-98ee-ed7e71e609d8" />

```python
model.evaluate(x_test_scaled, y_test)
```
<img width="818" height="40" alt="image" src="https://github.com/user-attachments/assets/487fb9a3-6ca0-43d6-ada4-887a0c155761" />

-----------------

# RNN(Recurrent Neural Network, 순환 신경망)


시계열 데이터, 자연어 처리 등에서 좋은 성능을 주는 신경망.


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import keras

from sklearn.model_selection import train_test_split
```

# IMDB 데이터셋


*   imdb 사이트 사용자들의 영화 리뷰를 긍정(1), 부정(0)으로 분류한 데이터.
*   25,000개 훈련 샘플과 25,000개 테스트 샘플.
*   샘플마다 토큰(단어)의 개수가 다름 - 샘플마다 특성(feature)의 개수가 다름. --> 전처리


```python
(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.imdb.load_data(num_words = 200)
# num_words: 가장 자주 등장하는 단어 n개를 어휘 사전으로 사용함.(가장 자주 나타나는 단어 200개만 사용함)
```
```python
x_train_full.shape   # 1차원 배열
```
<img width="68" height="27" alt="image" src="https://github.com/user-attachments/assets/0c9b8497-2f4b-4250-ba88-93805afc376d" />

```python
x_test.shape
```
<img width="67" height="27" alt="image" src="https://github.com/user-attachments/assets/3cffbf44-535b-45f8-991e-8de5056c41d4" />

```python
y_train_full.shape   # label은 항상 1차원
```
<img width="70" height="27" alt="image" src="https://github.com/user-attachments/assets/42222cee-2b3f-4ef2-aefd-72cd095273e6" />

```python
np.unique(y_train_full, return_counts = True)
# 훈련 셋에는 긍정/부정 리뷰가 각각 12,500개씩 포함.
```
```python
y_test.shape
```
<img width="68" height="26" alt="image" src="https://github.com/user-attachments/assets/8847c58b-832b-4a4f-b66c-fd9653640dc0" />

```python
np.unique(y_test, return_counts = True)
# 테스트 셋에서도 긍정/부정 리뷰가 각각 12,500개씩 포함.
```
<img width="279" height="26" alt="image" src="https://github.com/user-attachments/assets/bc3a6b06-54c7-443f-a082-a7a6258234e4" />

## 훈련 셋 탐색

```python
print(x_train_full[0])
```
```
[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 4, 173, 36, 2, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 2, 2, 5, 150, 4, 172, 112, 167, 2, 2, 2, 39, 4, 172, 2, 2, 17, 2, 38, 13, 2, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 2, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 2, 12, 8, 2, 8, 106, 5, 4, 2, 2, 16, 2, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 2, 28, 77, 52, 5, 14, 2, 16, 82, 2, 8, 4, 107, 117, 2, 15, 2, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 7, 4, 2, 2, 13, 104, 88, 4, 2, 15, 2, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 2, 22, 21, 134, 2, 26, 2, 5, 144, 30, 2, 18, 51, 36, 28, 2, 92, 25, 104, 4, 2, 65, 16, 38, 2, 88, 12, 16, 2, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
```
```python
print(type(x_train_full[0]))
```
<img width="116" height="27" alt="image" src="https://github.com/user-attachments/assets/f1777b8c-3172-422d-9cd3-e59d84755a20" />

```python
print(len(x_train_full[0]))
```
<img width="33" height="24" alt="image" src="https://github.com/user-attachments/assets/6769b257-f421-44c5-ace1-7171c26bb326" />


훈련 셋의 첫번째 샘플은 정수 218개를 갖는 리스트(list).


```python
print(x_train_full[1])
```
```
[1, 194, 2, 194, 2, 78, 2, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 2, 20, 13, 119, 2, 189, 102, 5, 2, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 2, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 2, 9, 2, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 2, 9, 45, 43, 38, 2, 2, 2, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 2, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 2, 165, 2, 98, 5, 4, 2, 9, 43, 2, 2, 15, 2, 120, 5, 120, 174, 11, 2, 175, 136, 50, 9, 2, 2, 2, 5, 2, 2, 2, 2, 5, 4, 2, 131, 152, 2, 18, 2, 32, 2, 2, 14, 9, 6, 2, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 2, 33, 89, 78, 2, 16, 145, 95]
```
```python
print(type(x_train_full[1]))
```
<img width="114" height="23" alt="image" src="https://github.com/user-attachments/assets/17f5e109-f4ee-4476-a950-17a1ffe4ebfc" />

```python
print(len(x_train_full[1]))
```
<img width="41" height="27" alt="image" src="https://github.com/user-attachments/assets/d258bc05-9044-464f-9ae6-5358a93c19d1" />


훈련 셋의 두번째 샘플은 정수 189개를 갖는 리스트(list).


imdb 데이터 셋의 훈련 셋은 파이썬의 **리스트(list) 객체**를 원소로 갖는 **1차원 np.ndarray** 타입.


```python
for i in range(5):
    print(f'인덱스-{i} 샘플의 토큰(단어) 개수: {len(x_train_full[i])}')
```
<img width="265" height="92" alt="image" src="https://github.com/user-attachments/assets/4bf6dbaa-782d-466c-af95-cc2f01b48c57" />

```python
for i in range(5):
    print(x_train_full[i][:20])
```
<img width="531" height="91" alt="image" src="https://github.com/user-attachments/assets/dd5896d3-b68a-4c78-9f49-504d3c047a8e" />


각각의 영화 리뷰의 길이(사용된 단어 개수)가 다르다. 1차원 리스트의 원소들은 영어 단어에 매칭된 숫자들.


```python
# word_index: 단어(word)를 키로 갖고, 그 단어의 인덱스를 값으로 갖는 dict.
# 영화 리뷰의 단어들을 숫자로 인코딩할 때 사용하기 위해서 만들어진 dict.
# 숫자 인덱스는 데이터셋에 등장하는 빈도수에 따라서 결정.(낮은 숫자가 빈도수가 더 큼)
word_index = keras.datasets.imdb.get_word_index()
```
<img width="729" height="40" alt="image" src="https://github.com/user-attachments/assets/4299e669-133c-4d21-8f20-49bb14831195" />

```python
print(type(word_index)) # 단어(word)를 키로 갖고, 그 단어의 인덱스를 값으로 갖는 dict.
```
<img width="113" height="27" alt="image" src="https://github.com/user-attachments/assets/29f73833-cafc-4242-8968-9636087b1f07" />

```python
word_index  # 어떤 단어가 어떤 숫자에 매칭되어 있는지 보여줌
```
<img width="150" height="192" alt="image" src="https://github.com/user-attachments/assets/2383feb1-4e28-4428-b7e3-c345a3e7a91c" />

```python
len(word_index)   # 88,584개 단어와 그 인덱스를 저장.
```
<img width="52" height="24" alt="image" src="https://github.com/user-attachments/assets/99bca560-b16f-40ad-bc74-e85ac5ad332d" />

```python
for i, (k, v) in enumerate(word_index.items()):
   print(k, v)
   if i == 5:
      break
```
<img width="107" height="113" alt="image" src="https://github.com/user-attachments/assets/12f7ae2a-3243-458d-8112-90cc530ddb1c" />


인덱스를 단어로 디코딩하기 위해서는 단어의 인덱스를 키로 하고 단어를 값으로 갖는 dict가 있었으면 좋겠다.


```python
index_word = {v:k for k, v in word_index.items()}
```
```python
for i, (k, v) in enumerate(index_word.items()):
    print(k, v)
    if i == 5:
        break
```
<img width="108" height="105" alt="image" src="https://github.com/user-attachments/assets/23d2076e-ddca-407d-a474-d913b1cb3440" />

```python
for i in range(1, 5):   # 0이 없음
    print(index_word.get(i))
# 젤 많이 쓰는게 the
```
<img width="38" height="72" alt="image" src="https://github.com/user-attachments/assets/7da4dc39-477f-41aa-ac28-568043c9b79d" />


각 단어의 인덱스들은 단어의 빈도수 순위를 의미. 인덱스 0은 패딩(padding)을 위한 숫자.


영화 리뷰 샘플의 숫자들의 의미:
*   0: 패딩(padding)
*   1: 문장의 시작.
*   2: load_data() 함수의 아규먼트 num_words에 포함되지 않은 단어들.
       (num_words=200를 사용하면, 데이터셋에서 가장 자주 나오는 단어 200개만 남기고 나머지는 무시됨. 이때, 무시된 단어들은 모두 숫자 2로 변환)
*   3: word_index (또는 index_word)의 인덱스. (실제 단어 인덱스는 3부터 시작)


```python
def decode_review(review):
    # review: 숫자들의 리스트
    return ' '.join([index_word.get(i-3, '?') for i in review])
```
```python
decode_review(x_train_full[0])
```
```
? this film was just ? ? ? ? story ? ? really ? the part they ? and you could just ? being there ? ? is an ? ? and now the same being director ? ? ? from the same ? ? as ? so i ? the fact there was a real ? with this film the ? ? ? the film were great it was just ? so much that i ? the film as ? as it was ? for ? and would ? it to ? to watch and the ? ? was ? really ? at the end it was so ? and you know what they say if you ? at a film it ? have been good and this ? was also ? to the two little ? that ? the ? of ? and ? they were just ? ? are ? ? out of the ? ? i think because the ? that ? them all ? up are such a big ? for the ? film but these ? are ? and should be ? for what they have ? don't you think the ? story was so ? because it was ? and was ? life after all that was ? with us all
```

1, 2까지는 ?로 출력된거임

```python
decode_review(x_train_full[1])
```
```
? big ? big ? bad ? and a ? ? ? these are the ? to best ? this ? movie i love ? horror movies and ? seen ? but this had got to be on of the ? ever made the plot is ? ? and ? the acting is an ? the ? is ? ? the best is the end ? with the ? and how he ? out who the ? is it's just so ? ? ? the ? are ? and funny in ? ? the ? is big ? of ? ? ? ? those ? ? ? that show off their ? ? that ? actually ? them and the ? is just ? ? that ? over and over again in ? every scene there is ? ? ? and ? ? ? ? and the ? still doesn't ? for ? all ? ? this is a ? bad film ? only ? is to look back on the ? that was the ? and have a good old ? at how bad ? was back then
```
