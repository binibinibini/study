# MNIST 데이터셋 KMeans

*   MNIST(숫자 손글씨 이미지들) 데이터셋 준비
*   KMeans 과정을 수행하면서 최적의 클러스터 개수를 찾음.
*   찾은 각 클러스터의 중심을 시각화

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn import datasets
```
```python
mnist = datasets.fetch_openml('mnist_784')
```
```python
print(type(mnist))  # Bunch 클래스 타입(파이썬의 dict과 비슷)
```
<img width="231" height="23" alt="image" src="https://github.com/user-attachments/assets/352a3803-a9ac-4576-b2c1-3c18daeb9ff7" />

```python
data = mnist.data.values.copy()
```
```python
data.shape  # (70_000, 784) - 70,000개의 784(28x28) 픽셀 이미지들
```
<img width="79" height="21" alt="image" src="https://github.com/user-attachments/assets/8bd0599e-06a1-4e3c-9cd9-e8a05e852a07" />

<img width="346" height="180" alt="image" src="https://github.com/user-attachments/assets/ce5177a8-ca76-478f-9cd2-8b6c470d4bb5" />

```python
inertias = []
k_values = np.arange(2, 21)
for k in k_values:
    kmeans = KMeans(n_clusters = k, random_state = 42)
    kmeans.fit(X = data)  # 해당하는 k값의 클러스트 중심을 찾는 거
    inertias.append(kmeans.inertia_)

print(inertias)
```
```
[224609793358.58237, 213604994897.13248, 205222223264.56873, 199762721443.60873, 192102859541.12604, 188104388015.0826, 184748228069.9557, 181170519577.72144, 178464482149.00037, 176073942860.336, 173987628543.6459, 171541193462.44077, 170265178558.73187, 167839842019.72528, 165807991237.28656, 164241430516.38873, 162818969725.16376, 161518409408.33966, 160203505242.69888]
```
```python
plt.plot(k_values, inertias, 'o-')
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```
<img width="455" height="355" alt="image" src="https://github.com/user-attachments/assets/8f16b459-524a-4cd4-994c-82f48ce3515e" />

<img width="255" height="204" alt="image" src="https://github.com/user-attachments/assets/0cf08d55-7498-4554-a839-3468753cb8e5" />

```python
k = 7일 때
kmeans = KMeans(n_clusters = 7, random_state = 42)
kmeans.fit(X = data)
centers = kmeans.cluster_centers_
```
```python
centers.shape
```
<img width="63" height="27" alt="image" src="https://github.com/user-attachments/assets/7e736ea8-5e52-4de0-a03d-9dff99ffd9f7" />

```python
fig, ax = plt.subplots(nrows = 1, ncols = 7, figsize = (14, 2))
for i in range(7):
    ax[i].imshow(centers[i].reshape(28, 28))
    ax[i].axis('off')
plt.show()
# 70000개를 7개의 클러스터로 나눈거.
```
<img width="890" height="124" alt="image" src="https://github.com/user-attachments/assets/6dcc2bc8-58b4-4cf2-bea5-2758cea7ccae" />

```python
# kmeans의 속도가 느려서 훈련속도를 빠르게 할수있는게 MiniBatchKMeans
from sklearn.cluster import MiniBatchKMeans
```
```python
inertias = []
k_values = np.arange(2, 30)
for k in k_values:
    mini_batch_kmeans = MiniBatchKMeans(n_clusters = k, batch_size = 64, random_state = 42)  # batch_size가 1이면 kmeans랑 같음. batch_size가 작을수록 속도가 느려짐
    mini_batch_kmeans.fit(X = data)
    inertias.append(mini_batch_kmeans.inertia_)  # 클러스터 내 거리의 제곱합. 작을수록 클러스터링이 잘 된 것
print(inertias)
```
```
[225057844336.61795, 213984177032.7945, 205646004740.98514, 201108626436.33063, 193805180924.64212, 190239377279.4798, 185268227831.83514, 184454036632.49463, 181309255080.62967, 177334925628.5763, 175752837734.01178, 172360261322.91257, 170368803716.96103, 169709706557.23694, 168029043770.77844, 166370690362.47696, 164391536126.13022, 163195656543.6248, 162088346159.41147, 162090466706.92603, 160053217102.94653, 159702591523.1783, 159574500119.92728, 157723425146.00748, 156158835050.68857, 155496274576.88232, 154599376118.00464, 153762374790.99496]
```
```python
plt.plot(k_values, inertias, 'o-')
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```
<img width="459" height="357" alt="image" src="https://github.com/user-attachments/assets/20be2555-63f6-4a33-a5b0-18fc651d1295" />

```python
# k = 10일 때
mini_batch_kmeans = MiniBatchKMeans(n_clusters = 10, batch_size = 64, random_state = 42)  # batch_size = 64는 한 번에 64개의 샘플만 사용해서 클러스터 중심을 업데이트 함
mini_batch_kmeans.fit(X = data)
centers = mini_batch_kmeans.cluster_centers_  # centers.shape -> (10, 784)
```
```python
fig, ax = plt.subplots(nrows = 1, ncols = 10, figsize = (14, 2))
for i in range(10):
    ax[i].imshow(centers[i].reshape(28, 28))
    ax[i].axis('off')
plt.show()
```
<img width="896" height="93" alt="image" src="https://github.com/user-attachments/assets/0ec89a28-c69d-45a5-8d1d-6c4881f351de" />


# 차원 축소, 주성분 분석(PCA, Principal Component Analysis)

<img width="726" height="320" alt="image" src="https://github.com/user-attachments/assets/8e98ab9a-0e18-4fad-992b-2dc99c6de1c8" />

<img width="728" height="323" alt="image" src="https://github.com/user-attachments/assets/f416f54e-0f1d-4e49-9270-054a22c564e7" />

<img width="724" height="318" alt="image" src="https://github.com/user-attachments/assets/7a9737ae-3996-40b7-950e-e7edcfc98a8c" />

<img width="706" height="367" alt="image" src="https://github.com/user-attachments/assets/6f37fdc0-2339-4319-a237-53f876c0c44e" />

#Imports

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
```

# 데이터셋

```python
!wget https://bit.ly/fruits_300 -O fruits.npy
```
<img width="749" height="285" alt="image" src="https://github.com/user-attachments/assets/999135fe-c85d-4bb8-894f-d0f9a5b90db2" />

```python
fruits = np.load('fruits.npy')  # npy 파일을 읽어서 numpy 배열을 생성
```
```python
fruits.shape
```
<img width="106" height="24" alt="image" src="https://github.com/user-attachments/assets/56316991-8662-4cba-b2b8-c8f812dc492d" />

```python
def plot_array(arr, ncols = 10):
    # arr: (samples, width, height) shape의 3차원 배열
    n = len(arr)  # 샘플 개수
    nrows = int(np.ceil(n / ncols))
    fig, ax = plt.subplots(nrows, ncols, figsize = (ncols, nrows))
    for i in range(nrows):
        for j in range(ncols):
            idx = i * ncols + j
            if nrows == 1:  # ax는 1차원 ndarray
                if idx < n:  # 샘플이 있는 경우
                    ax[idx].imshow(arr[idx], cmap = plt.cm.binary)
                ax[idx].axis('off')
            else:  # ax는 2차원 배열
                if idx < n:
                    ax[i, j].imshow(arr[idx], cmap = plt.cm.binary)
                ax[i, j].axis('off')
    plt.show()
```
```python
plot_array(fruits[:50])  # 0 ~ 50까지만 출
```
<img width="794" height="404" alt="image" src="https://github.com/user-attachments/assets/ed9d821a-8177-471b-a856-945df5c87d9e" />

```python
plot_array(fruits[:50], ncols = 6)
```
<img width="484" height="713" alt="image" src="https://github.com/user-attachments/assets/04c3752f-c752-4fab-a99d-b439dd4e0f46" />

```python
plot_array(fruits[:10])
```
<img width="794" height="85" alt="image" src="https://github.com/user-attachments/assets/cb628911-820a-4f92-99f5-49579ea5a824" />

# PCA

```
주성분 분석
*   훈련 셋에서 분산이 최대가 되는 축을 찾음.
*   첫번째 축에 수직이면서(직교하는) 남은 분산이 최대가 되는 축을 찾음.
*   이전에 찾은 두 축에 모두 수직이면서 남은 분산이 최대가 되는 축을 찾음.
*   위의 과정을 반복.
*   주성분 분석을 사용해서 차원을 축소(압축). 원래 차원으로 재구성할 수도 있음. 재구성 오차가 발생함
```
```python
# scikit-learn 클래스를 사용하기 위해서 3d ndarray를 2d ndarray로 변환(scikit-learn는 2차원으로 받아야함)
fruits_2d = fruits.reshape((300, -1))
```
```python
fruits_2d.shape  # (samples, pixels)
```
<img width="81" height="22" alt="image" src="https://github.com/user-attachments/assets/34b54aa2-fedd-48f7-bf11-b6573f0d44f1" />

```python
# PCA 클래스 객체 생성
pca = PCA(n_components = 50, random_state = 42)  # 주성분 벡터를 50개 선택.(분산이 큰 순서대로 정렬된 50개의 벡터를 구하는 것)
```
```python
# 비지도 학습(데이터셋 훈련) -> 50개의 주성분을 찾음.
pca.fit(X = fruits_2d)
```
<img width="239" height="59" alt="image" src="https://github.com/user-attachments/assets/34512096-43ed-42c5-8226-7bb9688f5278" />

```python
# PCA에서 찾은 50개의 주성분
pca.components_.shape   # (n_components, n_features). 50개의 100x100 이미지
# pca.components_[0]는 첫 번째 주성분 벡터(가장 분산이 큰 방향)
```
<img width="75" height="20" alt="image" src="https://github.com/user-attachments/assets/7691dce0-1118-4926-9af2-7bd29fd0e8a2" />

```python
plot_array(pca.components_.reshape((50, 100, 100)))
```
<img width="794" height="404" alt="image" src="https://github.com/user-attachments/assets/15defb7f-f195-420a-997c-0bea07fe4892" />

```
scikit-learn
- estimator class: fit, predict, score
- transformer class: fit, transform, fit_transform
```

## 차원 축소

`PCA.transform()` 메서드 호출.

```python
fruits_2d_reduced = pca.transform(X = fruits_2d)
```
```python
fruits_2d_reduced.shape  # (n_samples, n_components) 
```
<img width="68" height="25" alt="image" src="https://github.com/user-attachments/assets/3fbc57d7-118f-4121-b639-df2f6df4c30e" />

## 차원 재구성: 축소됐던 차원을 원래 차원으로 되돌림.

`PCA.inverse_transform()` 메서드 호출.

```python
fruits_2d_reconst = pca.inverse_transform(X = fruits_2d_reduced)
```
```
fruits_2d_reconst.shape     # (n_samples, n_features)
```
<img width="83" height="22" alt="image" src="https://github.com/user-attachments/assets/4409a273-575a-437f-b660-8bfb5950be7f" />

```
원본 과일 이미지와 축소했다가 재구성한 이미지를 비교
```
```python
fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 4))

# 원본 이미지
ax[0].imshow(fruits[0], cmap = plt.cm.binary)
ax[0].set_title('original')

# 재구성된 이미지
ax[1].imshow(fruits_2d_reconst[0].reshape(100, 100), cmap = plt.cm.binary)  # reshape(100, 100)는 1차원 배열을 2차원 이미지 형태로 바꾸는 작업. imshow()는 2차원 배열을 받아야 그릴 수 있음
ax[1].set_title('reconstructed')

plt.show()
```
<img width="667" height="347" alt="image" src="https://github.com/user-attachments/assets/ddb23e4f-d07d-4bae-a505-46e9d5066966" />

```python
fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 4))

# 원본 이미지
ax[0].imshow(fruits[100], cmap = plt.cm.binary)
ax[0].set_title('original')

# 재구성된 이미지
ax[1].imshow(fruits_2d_reconst[100].reshape(100, 100), cmap = plt.cm.binary)
ax[1].set_title('reconstructed')

plt.show()
```
<img width="667" height="347" alt="image" src="https://github.com/user-attachments/assets/392938d3-4a45-465c-b380-977d810b919f" />

```python
fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 4))

# 원본 이미지
ax[0].imshow(fruits[200], cmap = plt.cm.binary)
ax[0].set_title('original')

# 재구성된 이미지
ax[1].imshow(fruits_2d_reconst[200].reshape(100, 100), cmap = plt.cm.binary)
ax[1].set_title('reconstructed')

plt.show()
```

# 설명된 분산 비율(Explained Variance Ratio)

데이터 셋의 분산이 각 주성분 축에 따라 놓여 있는 비율.

```python
pca.explained_variance_  # 각 주성분 벡터가 설명하는 분산의 크기. 주성분이 50개니깐 총 50개
```
<img width="404" height="244" alt="image" src="https://github.com/user-attachments/assets/59d59ae8-3dae-4f6a-a56c-91fa87dd2b37" />

```
pca.explained_variance_ratio_  # 설명된 분산 비율(0 ~ 1). 첫번째 축은 전체 분산의 42%를 설명하고 있는거 (각 주성분 벡터가 전체 데이터의 분산 중에서 차지하는 비율)
```
<img width="415" height="145" alt="image" src="https://github.com/user-attachments/assets/1df7e78d-101d-4ddf-bdec-c0a5ab9b3ad7" />

```python
np.sum(pca.explained_variance_ratio_)   # 단순 합계
```
<img width="190" height="22" alt="image" src="https://github.com/user-attachments/assets/d8e30a39-155f-43ef-a4da-36334c646bb3" />

```python
np.cumsum(pca.explained_variance_ratio_)  # 누적 합계
```
<img width="418" height="150" alt="image" src="https://github.com/user-attachments/assets/27380f15-67cf-496b-b5d0-7a32fc72b7e4" />

```python
# 설명된 분산 비율을 y측으로 그린 선 그래프
x = np.arange(1, 51)
y = np.cumsum(pca.explained_variance_ratio_)
plt.plot(x, y)
plt.grid()
plt.xlabel('components')
plt.ylabel('exaplained variance ratio')
plt.show()
```
<img width="567" height="432" alt="image" src="https://github.com/user-attachments/assets/af5b7185-d1f4-4f99-923f-34ab446ad23f" />

## 적절한 차원(주성분)의 개수 찾기.

```python
pca2 = PCA(random_state = 42)   # n_components 파라미터를 설정하지 않은 PCA 객체 생성
```
```python
pca2.fit(X = fruits_2d)     # 학습 -> 주성분 찾음.
```
<img width="146" height="59" alt="image" src="https://github.com/user-attachments/assets/c6bfe30b-481b-40ce-aeed-6742c595ddf7" />

```python
pca2.components_.shape      # PCA으로 찾은 주성분 축(axis) 배열. (차원이 하나도 축소가 되지 않았음)
```
<img width="80" height="22" alt="image" src="https://github.com/user-attachments/assets/e0f645d0-531b-47ec-bf6c-1250b9cf69a9" />

```python
x = np.arange(1, 301)   # 주성분 개수
y = np.cumsum(pca2.explained_variance_ratio_)   # 설명된 분산 비율들의 누적 합계
plt.plot(x, y)
plt.grid()
plt.xlabel('components')
plt.ylabel('explained variance ratio')
plt.show()
```
<img width="567" height="432" alt="image" src="https://github.com/user-attachments/assets/8408eb66-4f9e-4c23-8415-5e168ed659e1" />

```
PCA 객체를 생성할 때 n_components 파라미터를 설명된 분산비율(0.0 ~ 1.0)로 설정할 수도 있음.
```
```python
pca3 = PCA(n_components = 0.95, random_state = 42)
pca3.fit(X = fruits_2d)
pca3.components_.shape
# (74, 10000) -> 74 : PCA가 선택한 주성분의 개수(95% 분산을 설명하기 위해 필요한 수). 10000 : 원래 데이터의 특성 수
```
<img width="72" height="24" alt="image" src="https://github.com/user-attachments/assets/fd50c9b8-e90d-478c-a0c8-0fa1900996e5" />

```
설명된 분산 비율이 전체 데이터 분산의 95%인 주성분의 개수는 74개.
```
```python
fruits_2d_reduced = pca3.transform(X = fruits_2d)   # 차원 축소
fruits_2d_reduced.shape
```
<img width="66" height="22" alt="image" src="https://github.com/user-attachments/assets/c0c6fc1b-095b-4c3c-9d8f-3e8d4a767b29" />

```python
fruits_2d_reconst = pca3.inverse_transform(X = fruits_2d_reduced)   # 차원 재구성
fruits_2d_reconst.shape
```
<img width="79" height="24" alt="image" src="https://github.com/user-attachments/assets/87f4a4ac-ea54-4b2b-a33a-1418789050f2" />

```python
fig, ax = plt.subplots(ncols = 2, figsize = (8, 4))
ax[0].imshow(fruits[100], cmap = plt.cm.binary)
ax[1].imshow(fruits_2d_reconst[100].reshape((100, 100)), cmap = plt.cm.binary)
plt.show()
```
<img width="667" height="329" alt="image" src="https://github.com/user-attachments/assets/2f701f7a-839f-4a99-b0b3-a2a2de9536a1" />

# PCA와 지도학습(분류)

## 차원 축소 없이 분류

```python
# 지도학습에 필요한 타켓 배열
target = np.array([0] * 100 + [1] * 100 + [2] * 100)    # 0이 100개, 1이 100개, 2가 100개인 타겟 배열
```
```python
# LogisticRegression을 사용한 분류

# 코드 실행 시간을 측정해서 출력
%%time
cv = cross_validate(estimator = LogisticRegression(random_state = 42),  # ML 모델
                    x = fruits_2d,  # 특성 배열
                    y = target,      # 타겟 배열
                    return_train_score = True)
# 교차검증
```
<img width="317" height="39" alt="image" src="https://github.com/user-attachments/assets/9be04a08-1748-4835-8833-1cdf2507311d" />

```python
type(cv)  # dict타입이라서 key, value가 있음
```
<img width="35" height="21" alt="image" src="https://github.com/user-attachments/assets/763037fd-5a7c-43e1-bd70-843c94650fcf" />

```python
cv.keys()  # 어떤 항목들이 포함되어 있는지 보여주는 명령어
```
<img width="409" height="24" alt="image" src="https://github.com/user-attachments/assets/9eb58ee7-91e9-4902-8f66-c2fd28ffcb1c" />

```python
cv['train_score']   # 훈련셋에서 정확도
```
<img width="170" height="23" alt="image" src="https://github.com/user-attachments/assets/4354cb23-bb83-4c9a-b817-a25a3a3adc05" />

```python
np.mean(cv['train_score'])
```
<img width="100" height="23" alt="image" src="https://github.com/user-attachments/assets/9e49a6b8-5bda-4964-b211-5c6edee48a1d" />

```python
cv['test_score']
```
<img width="416" height="27" alt="image" src="https://github.com/user-attachments/assets/015cf8d0-d746-4adb-a497-68efce338a10" />

```python
np.mean(cv['test_score'])
```
<img width="194" height="27" alt="image" src="https://github.com/user-attachments/assets/05b7d590-9be1-4958-b1c4-9dc90f563180" />

## PCA로 차원 축소한 후 분류

### 95% 설명된 분산 비율

```python
pca = PCA(n_components = 0.95, random_state = 42)   # 분산 비율 95%
```
```python
pca.fit(X = fruits_2d)
```
<img width="256" height="63" alt="image" src="https://github.com/user-attachments/assets/131cc7d1-1577-438a-b776-886683e7773a" />

```python
pca.n_components_     # PCA가 찾은 주성분의 갯수(10000개를 74개로 줄일수 있다는 거)
```
<img width="85" height="20" alt="image" src="https://github.com/user-attachments/assets/e5f82db6-e4ae-4c3e-b83e-535c0baa624e" />

```python
fruits_2d_reduced = pca.transform(X = fruits_2d)    # 74개의 주성분으로 차원 축소 - (300, 74)
```
```python
# 교차 검증
%%time
cv = cross_validate(estimator = LogisticRegression(random_state = 42),
                    X = fruits_2d_reduced,
                    y = target,
                    return_train_score = True)
```
<img width="300" height="36" alt="image" src="https://github.com/user-attachments/assets/2ec08e94-7150-4e1d-919c-93269ab4e876" />

```python
np.mean(cv['train_score'])      # 훈련 점수
```
<img width="100" height="25" alt="image" src="https://github.com/user-attachments/assets/3af52a08-409b-4843-b85f-9013c29b19ab" />

```python
np.mean(cv['test_score'])       # 검증 점수
```
<img width="192" height="24" alt="image" src="https://github.com/user-attachments/assets/a790cce1-2cac-4bcd-8966-9808a80ffc4b" />

### 50% 설명된 분산 비율

```python
pca = PCA(n_components = 0.5, random_state = 42)    # PCA 객체 생성
pca.fit(X = fruits_2d)  # 주성분 찾기
fruits_2d_reduced = pca.transform(X = fruits_2d)    # 차원 축소 - (300, 2)
```
```python
pca.n_components_   # 50% 분산을 설명하는 주성분 개수는 2개
```
<img width="85" height="25" alt="image" src="https://github.com/user-attachments/assets/51c05d16-5209-4645-9882-3adccc8cfb2b" />

```python
%%time
cv = cross_validate(estimator = LogisticRegression(random_state = 42),
                    X = fruits_2d_reduced,
                    y = target,
                    return_train_score = True)
```
<img width="829" height="143" alt="image" src="https://github.com/user-attachments/assets/467b5752-d11c-4166-98fc-a25920c25f4a" />

```python
np.mean(cv['train_score'])
```
<img width="107" height="22" alt="image" src="https://github.com/user-attachments/assets/a2d5a56f-c203-4991-8030-c5eb0e6c495b" />

```python
np.mean(cv['test_score'])
```
<img width="197" height="27" alt="image" src="https://github.com/user-attachments/assets/6fb22253-c5e8-4b08-abbb-430728ae2111" />

```python
labels = ['apple', 'pineapple', 'banana']
for i, cls in enumerate(labels):
    plt.scatter(x = fruits_2d_reduced[i*100 : (i+1)*100, 0],  # 각 클래스당 100개 샘플을 슬라이싱
                y = fruits_2d_reduced[i*100 : (i+1)*100, 1],
                label = cls)
# x, y -> PCA의 첫 번째 주성분과 두 번째 주성분을 x, y축에 사용. 0, 1은 PCA로 축소된 2차원 데이터의 각 축을 의미. [:,0]->첫 번째 주성분(x축 좌표)
plt.legend()
plt.grid()
plt.xlabel('c1')
plt.ylabel('c2')
plt.show()
```
<img width="599" height="432" alt="image" src="https://github.com/user-attachments/assets/2f584fb3-7bec-45c1-b084-8ace0057b714" />



